{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05 - Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfUIPsyfU43B",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "\n",
        "Now that you understand the intuition behind CNNs as well as the key building components, it is time to get some in-depth hands-on experience for training a network (making it learn). The key is to understand the effects of **hyperparameters** on network training and validation accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efi-AKsFXGR7",
        "colab_type": "text"
      },
      "source": [
        "## Restarting your Virtual Machine\n",
        "â€‹\n",
        "If at any point during this assignment you accidentally execute code or do something that cannot seem to undo and need to \"restart\" the system (including deleting all temporary folders), go ahead and run the following single line of code. It will take about 1 minute to restart. Following this, you will have to proceed at the beginning of the assignment to re-downloaded the data and run the code you have written. Note, the code that you have already written will **not** be deleted; you simply need to start executing the code once again from the start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feYxnR_yXIJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1 # Warning this restarts your machine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdhVLlM0XLEK",
        "colab_type": "text"
      },
      "source": [
        "## Downloading the Data\n",
        "\n",
        "Since we have just about mastered the MNIST dataset, it's time to move on to something (slightly) more challenging. In this assignemnt we will be using the CIFAR dataset. Let's download and import it now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TypLEulDXOmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "(x, y), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANKQf7efXUye",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Data\n",
        "\n",
        "The CIFAR dataset consists of many small thumbnail RGB images (32 x 32 x 3) across many classes of everyday objects. In this assignment we will specifically be using a subset of CIFAR known as CIFAR-10 containing just 10 classes: airplanes, autos, birds, cats, deer, dogs, frogs, horses, ships, and trucks. For each class, 6,000 examples are provided for a total of 60,000 images (50,000 for training, 10,000 for validation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sApZIwCNXW_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "import numpy as np\n",
        "import pylab\n",
        "\n",
        "# View random 16 examples\n",
        "fig = pylab.figure()\n",
        "for i in range(16):\n",
        "    index = np.random.randint(50000)\n",
        "    im = x[index]\n",
        "    fig.add_subplot(4, 4, i + 1)\n",
        "    pylab.imshow(im)\n",
        "    pylab.axis('off')\n",
        "    \n",
        "pylab.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAZ5nXNM7-T8",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "Recall that while **parameters** represent the actual values of the trainable weights, the term **hyperparameters** represents values that dictate the strategy for weight training. In the previous few assignments, we have already been exposed to various *optimizer types* and initial *learning rates*. Recall that generally the syntax is:\n",
        "```\n",
        "train_op = tf.train.[OptimizerType](learning_rate).minimize(loss)\n",
        "```\n",
        "Here the `OptimizerType` represents the particular strategy of optimization (AdamOptimizer used in many previous assignments by default) and learning_rate represents a floating point value, commonly starting between 1e-2 and 2e-4. Additionally recall that the *batch size* simply referred to the number of images used in each iteration to determine the update step. While a smaller batch size will lead to faster updates (more steps per unit time) a larger batch size will lead to more accurate updates.\n",
        "\n",
        "![Batch Size](https://raw.githubusercontent.com/CAIDMRes/images/master/assignment_06-01.png)\n",
        "\n",
        "## Preventing Overfitting\n",
        "\n",
        "While optimizer type, learning rate and batch size are commonly used hyperparameters to tune for maximal optimization, quite often an algorithm will in fact become *too accurate* and require separate hyperparameters or strategies to prevent overfitting. Recall that one easy strategy that we have already been exposed to in earlier lessons is the concept of *early stopping*. Several others that we will discuss include L2 regularization, dropout and batch normalization.\n",
        "\n",
        "### L2 regularization\n",
        "\n",
        "L2 regularization is strategy to limit the reliance on any given neuron. It is implemented by adding a second term to the loss function whereby the optimizer concurrently attempts to limit the squared magnitude of all weights in the network (in addition to trying to find weights that will result in high network accuracy / low loss).\n",
        "\n",
        "![L2 Regularization](https://raw.githubusercontent.com/CAIDMRes/images/master/assignment_06-02.png)\n",
        "\n",
        "Practically, this is implemented by simply taking the sum of all weights squared, then multiplying that total by a constant that indicates how much the loss function should be affected by the L2 regularization component. Recall that the `**` operator means raised to the exponent in Python. Also keep in mind that `tf.reduce_sum()` is the method within Tensorflow to take all elements in a tensor, add them all up, and return the sum total (similar to `np.sum()` in Numpy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep7g7_w8BTZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "w1 = tf.placeholder(tf.float32, [5, 5, 1, 16])\n",
        "w2 = tf.placeholder(tf.float32, [5, 5, 16, 32])\n",
        "\n",
        "# Define L2 regularization\n",
        "l2_reg = tf.reduce_sum(w1 ** 2) + tf.reduce_sum(w2 ** 2) \n",
        "\n",
        "# Determine weighting of L2 regularization\n",
        "l2_reg_constant = 0.1   # Use a less of L2 regularization\n",
        "l2_reg_constant = 5.0   # Use a more of L2 regularization\n",
        "l2_reg = l2_reg * l2_reg_constant\n",
        "\n",
        "print(l2_reg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhUbmBmrBNYU",
        "colab_type": "text"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "Dropout is another strategy to limit the reliance on any given neuron. The idea is that if an algorithm's prediction is being driven almost entire by one neuron / image feature (e.g. one portion of the image) it is unlikely to be a pattern that is generalizable to other images. By randomly \"turning off\" neurons during the training process, the algorithm is never allowed to rely too much on any given neuron output. In the figure below, the neurons marked in red are \"turned off\" and do *not* contribute to the final prediction.\n",
        "\n",
        "![Dropout](https://raw.githubusercontent.com/CAIDMRes/images/master/assignment_06-03.png)\n",
        "\n",
        "Recall that while the technique can be used anywhere in your network, it is most commonly used in the fully connected (non-convolutional) portion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjBoRDrIEGKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define an aribtrary feature map and first hidden layer weights\n",
        "feature_map = tf.placeholder(tf.float32, [None, 8, 8, 32])\n",
        "w1 = tf.placeholder(tf.float32, [8 * 8 * 32, 128])\n",
        "\n",
        "# Reshape and matmul\n",
        "feature_map = tf.reshape(feature_map, shape=[-1, 8 * 8 * 32])\n",
        "hidden = tf.matmul(feature_map, w1)\n",
        "dropped = tf.nn.dropout(hidden, keep_prob=0.75) # 75% of neurons kept / 25% dropped\n",
        "\n",
        "print(dropped.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz-SWRzdFpsO",
        "colab_type": "text"
      },
      "source": [
        "### Batch Normalizaion\n",
        "\n",
        "Batch normalization is a technique where by the feature maps for *all* images in the batch become normalized with each other (a single mean / SD for the entire batch). As you recall, this technique makes it such that the behavior of the network in prediction for an image is slightly altered depending on the other images in the batch (e.g. an image will be interpreted slightly differently every tie it is passed to the network with a different batch). This overall increases the \"diversity\" of your training example.\n",
        "\n",
        "The only thing to keep in mind is that while **during training** the mean and SD of the current batch is used at each layer, **during inference (or validation)** the *population* mean and SD is used instead. This makes sense in that during inference or validation you may occasionally want to test an image by itself in isolation, but in such a situation no other images would be available to calculate a mini-batch mean and SD. So to get around this, the algorithm will keep track of the average mean and SD of feature maps for all images used for training, and simply use these values to approximaate a \"virtual\" mini-batch for the purposes of batch normalization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VhqQGhQHh10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define an arbitrary input image and first convolutional weights\n",
        "im = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
        "w1 = tf.placeholder(tf.float32, [3, 3, 3, 16])\n",
        "\n",
        "# Convolution and batch-norm\n",
        "output = tf.nn.conv2d(im, filter=w1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "print(output.shape)\n",
        "output = tf.layers.batch_normalization(output, training=True)   # During training\n",
        "print(output.shape)\n",
        "output = tf.layers.batch_normalization(output, training=False)  # During validaion\n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oedca0HJb1Os",
        "colab_type": "text"
      },
      "source": [
        "# Training CIFAR-10\n",
        "\n",
        "It's now time to put together a basic CNN to tackle the CIFAR-10 dataset. Let us first define a basic building block for our network:\n",
        "\n",
        "Block =\n",
        "\n",
        "* convolution with 3 x 3 filter, stride 1\n",
        "* ReLU non-linearity\n",
        "* convolution with 3 x 3 filter, stride 2\n",
        "* ReLU non-linearity\n",
        "\n",
        "Based off of this definition, let us create a CNN with the following architecture:\n",
        "\n",
        "* Block 1 (filter sizes of 32) - output 16 x 16 x 32\n",
        "* Block 2 (filter sizes of 64) - output 8 x 8 x 64\n",
        "* Reshape - output 4096\n",
        "* Hidden layer - output 128\n",
        "* Logits - output 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egxDBjKqcIKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    \"\"\"\n",
        "    Method to create the following CNN architecture:\n",
        "    \n",
        "      - BLOCK 1 (filter depth of 32)\n",
        "      - BLOCK 2 (filter depth of 64)\n",
        "      - RESHAPE\n",
        "      - HIDDEN LAYER (128 nodes)\n",
        "      - LOGIT SCORES (10 nodes)\n",
        "      \n",
        "    \"\"\"\n",
        "    # Reset our graph to build a new one\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define placeholders for our images and labels\n",
        "    # ------------------------------------------------------------------------\n",
        "    #\n",
        "    # 1. Define images\n",
        "    # \n",
        "    #  - type: float32\n",
        "    #  - size: [None, 32, 32, 3] so that we can feed in as many images as we need\n",
        "    # \n",
        "    # 2. Define labels\n",
        "    # \n",
        "    #  - type: int64\n",
        "    #  - size: [None] so that we can feed in as many labels as we need\n",
        "    # \n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    im = tf.placeholder(?)\n",
        "    labels = tf.placeholder(?)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define convolutional blocks \n",
        "    # ------------------------------------------------------------------------\n",
        "    #\n",
        "    # As in previous assignments, we will use the tf.get_variables(...) method\n",
        "    # to create matrix variables initialized to random values.\n",
        "    # \n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Block 1 (use a filter depth of 32)\n",
        "    w1 = tf.get_variable('w1', ?, dtype=tf.float32)\n",
        "    w2 = tf.get_variable('w2', ?, dtype=tf.float32)\n",
        "    layer = tf.nn.relu(tf.nn.conv2d(im, w1, strides=?, padding='SAME'))\n",
        "    layer = tf.nn.relu(tf.nn.conv2d(layer, w2, strides=?, padding='SAME'))\n",
        "\n",
        "    # Block 2 (use a filter depth of 64)\n",
        "    w3 = tf.get_variable('w3', ?, dtype=tf.float32)\n",
        "    w4 = tf.get_variable('w4', ?, dtype=tf.float32)\n",
        "    layer = tf.nn.relu(tf.nn.conv2d(layer, w3, strides=?, padding='SAME'))\n",
        "    layer = tf.nn.relu(tf.nn.conv2d(layer, w4, strides=?, padding='SAME'))\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Reshape to 1D vector \n",
        "    # ------------------------------------------------------------------------\n",
        "    \n",
        "    flattened = tf.reshape(layer, [-1, 8 * 8 * 64])\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our matmul operations\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Hidden layer (128 nodes)\n",
        "    w5 = tf.get_variable('w5', ?, dtype=tf.float32)\n",
        "    h1 = tf.nn.relu(tf.matmul(flattened, w5))\n",
        "    \n",
        "    # Logits (10 nodes)\n",
        "    w6 = tf.get_variable('w6', ?, dtype=tf.float32)\n",
        "    logits = tf.matmul(h1, w6)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our softmax cross-entropy loss\n",
        "    # ------------------------------------------------------------------------\n",
        "    # \n",
        "    # HINT: use tf.losses.sparse_softmax_cross_entropy() as above\n",
        "    #\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our optimizer\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
        "    \n",
        "    return im, labels, [w1, w2, w3, w4, w5, w6], logits, loss, train_op\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Test our model\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# If the graph was defined properly, we should be able to check the out\n",
        "# what the model outputs should look like. Can you guess by the shapes\n",
        "# of our logits and losses will be?\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "im, labels, weights, logits, loss, train_op = create_model()\n",
        "print(logits.shape)\n",
        "print(loss.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yeQFY_Rjn5u",
        "colab_type": "text"
      },
      "source": [
        "## Intialization\n",
        "\n",
        "Now we set up some code to initialize our network graph, variables and new saver object. This code is identical to the earlier assignments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoKTxo5LcJYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Create our model\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "im, labels, weights, logits, loss, train_op = create_model()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Add to collections\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# Collections are used by TensorFlow to keep track of certain intermediate \n",
        "# values for quick access during save/load functions.\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "tf.add_to_collection('im', im)\n",
        "tf.add_to_collection('logits', logits)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Initialize our test graph\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# What two things do we need to initialize our graph?\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Initialize our test graph\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# Initialize a Saver object\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFF7d5-kjyzm",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Let's train our algorithm!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuXgVOYzcPag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Train our algorithm \n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# Let's set up a loop to train our algorithm by feeding it data iteratively.\n",
        "# For each iteration, we will feed a batch_size number of images into our \n",
        "# model and let it readjust it's neuronal weights.\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "def train_model(iterations=1000, batch_size=256):\n",
        "    \n",
        "    accuracies = []\n",
        "    losses = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Grab a total of batch_size number of random images and labels \n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # 1. Pick batch_size number of random indices between 0 and 50,000\n",
        "        # 2. Select those images / labels\n",
        "        #\n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        rand_indices = np.random.randint(?, size=(batch_size))\n",
        "        x_batch = x[?]\n",
        "        y_batch = y[?]\n",
        "       \n",
        "        y_batch = y_batch[..., 0]\n",
        "      \n",
        "        # --------------------------------------------------------------------\n",
        "        # Normalize x_batch\n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # Currently, values in x range from 0 to 255. If we normalize these values\n",
        "        # to a mean of 0 and SD of 1 we will improve the stability of training\n",
        "        # and furthermore improve interpretation of learned weights. Use the\n",
        "        # following code to normalize your batch:\n",
        "        # \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        x_batch = (x_batch - np.mean(x_batch)) / np.std(x_batch)\n",
        "\n",
        "        # Convert to types matching our defined placeholders\n",
        "        x_batch = x_batch.astype('float32')\n",
        "        y_batch = y_batch.astype('int64')\n",
        "\n",
        "        # Prepare feed_dict\n",
        "        feed_dict = {?}\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Run training iteration via sess.run()\n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # Here, in addition to whichever ouputs we wish to extract, we need to\n",
        "        # also include the train_op variable. Including train_op will tell \n",
        "        # Tensorflow that in addition to calculating the intermediates of our graph,\n",
        "        # we also need to readjust the variables so that the overall loss goes\n",
        "        # down.\n",
        "        # \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        outputs = sess.run([logits, loss, train_op], feed_dict)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Use argmax to determine highest logit (model guess)\n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # Keep in mind our logits matrix is (batch_size x 10) in size representing\n",
        "        # a total of batch_size number of predictions. How do we process this matrix\n",
        "        # with the np.argmax() to find the highest logit along each row of the matrix\n",
        "        # (e.g. find the prediction for each of our images)?\n",
        "        # \n",
        "        # HINT: what does the axis parameter in np.argmax(a, axis) specify?\n",
        "        # \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        predictions = np.argmax(?)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Calculate accuracy \n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # Consider the following:\n",
        "        # \n",
        "        # - predictions = the predicted digits\n",
        "        # - y_batch = the ground-truth digits\n",
        "        # \n",
        "        # How do I calculate an accuracy % with this data?\n",
        "        # \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        accuracy = ?\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Accumulate and print iteration, loss and accuracy \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        print('Iteration %05i | Loss = %07.3f | Accuracy = %0.4f' %\n",
        "            (i + 1, outputs[1], accuracy))\n",
        "\n",
        "        losses.append(outputs[1])\n",
        "        accuracies.append(accuracy)\n",
        "        \n",
        "    return losses, accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdgptfdWcVB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------------------------------------------------------\n",
        "# Train model\n",
        "# --------------------------------------------------------------------\n",
        "losses, accuracies = train_model(iterations=1000, batch_size=256)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Graph outputs and accuracy\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "import pylab\n",
        "pylab.plot(losses)\n",
        "pylab.title('Model loss over time')\n",
        "pylab.show()\n",
        "\n",
        "pylab.plot(accuracies)\n",
        "pylab.title('Model accuracy over time')\n",
        "pylab.show()\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Save model\n",
        "# --------------------------------------------------------------------\n",
        "# \n",
        "# In this step, all model variables and the underlying graph structure\n",
        "# are saved so that they can be reloaded. Although it looks like just one\n",
        "# file is saved here, in fact both a *.cpkt and *.cpkt.meta file are both\n",
        "# saved in this single line of code.\n",
        "#  \n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "model_file = './model_cnn_32_64_128/model.ckpt'\n",
        "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
        "print('Saving model')\n",
        "saver.save(sess, model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovOl0JE1cdxG",
        "colab_type": "text"
      },
      "source": [
        "# Running inference\n",
        "\n",
        "Now that we have a trained model, let's go ahead and see how it performs! We will use the same procedure as before to load up a trained network and then feed in random digits to see how it fares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnUhBDI2chqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the saved model\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "saver = tf.train.import_meta_graph('./model_cnn_32_64_128/model.ckpt.meta')\n",
        "saver.restore(sess, './model_cnn_32_64_128/model.ckpt')\n",
        "\n",
        "# Find our placeholders\n",
        "im = tf.get_collection('im')[0]\n",
        "logits = tf.get_collection('logits')[0]\n",
        "\n",
        "# Find a random test image\n",
        "i = int(np.random.randint(50000))\n",
        "image = x[i].reshape(1, 32, 32, 3)\n",
        "label = y[i, 0]\n",
        "\n",
        "# Normalize the image\n",
        "image = (image - np.mean(image)) / np.std(image)\n",
        "\n",
        "# Create a feed_dict\n",
        "feed_dict = {im: image}\n",
        "\n",
        "# Pass data through the network\n",
        "l = sess.run(logits, feed_dict)\n",
        "\n",
        "# Convert logits to predictions\n",
        "prediction = np.argmax(l)\n",
        "\n",
        "# Pred dictionary\n",
        "pred_dict = {\n",
        "    0: 'airplane',\n",
        "    1: 'automobile',\n",
        "    2: 'bird',\n",
        "    3: 'cat',\n",
        "    4: 'deer',\n",
        "    5: 'dog',\n",
        "    6: 'frog',\n",
        "    7: 'horse',\n",
        "    8: 'ship',\n",
        "    9: 'truck'\n",
        "}\n",
        "\n",
        "# Visualize\n",
        "pylab.imshow(x[i])\n",
        "pylab.axis('off')\n",
        "pylab.title('My prediction is %s' % pred_dict[prediction])\n",
        "pylab.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oypH0QDpcnGY",
        "colab_type": "text"
      },
      "source": [
        "# Model validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZreX2Sec4fC",
        "colab_type": "text"
      },
      "source": [
        "Using the template code to run inference shown above, let us now write code to:\n",
        "\n",
        "* load our saved model \n",
        "* create a `feed_dict` with new test data \n",
        "* pass through network using `sess.run()`\n",
        "* convert the `logits` to predictions\n",
        "* calculate overall network accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrdaSgVtc8Sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_model(model_file):\n",
        "    \"\"\"\n",
        "    Method to test the validation performance of a model using the \n",
        "    test set data.\n",
        "    \n",
        "    :params\n",
        "    \n",
        "      (str) model_file : name of model file saved by saver object\n",
        "      \n",
        "    \"\"\"\n",
        "    # Load saved model\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.InteractiveSession()\n",
        "    saver = tf.train.import_meta_graph('%s.meta' % model_file )\n",
        "    saver.restore(sess, model_file)\n",
        "\n",
        "    # Find our placeholders\n",
        "    im = tf.get_collection('im')[0]\n",
        "    logits = tf.get_collection('logits')[0]\n",
        "\n",
        "    # Normalize our input data x_test\n",
        "    input_data = (x_test - np.mean(x_test)) / np.std(x_test)\n",
        "    \n",
        "    # -------------------------------------------------------\n",
        "    # Create a feed_dict\n",
        "    # -------------------------------------------------------\n",
        "    # \n",
        "    # HINT: What do we need to do to properly format this image\n",
        "    # for input into the CNN?\n",
        "    # \n",
        "    # -------------------------------------------------------\n",
        "\n",
        "    feed_dict = {?}\n",
        "\n",
        "    # Pass data through the network using sess.run() to get our logits \n",
        "    output = sess.run(?)\n",
        "\n",
        "    # Convert logits to predictions\n",
        "    predictions = np.argmax(output, axis=1)\n",
        "\n",
        "    # Compare predictions to ground-truth to find accuracy\n",
        "    accuracy = np.sum(predictions == y_test[..., 0]) / x_test.shape[0]\n",
        "    \n",
        "    print('Network test-set accuracy: %0.4f' % accuracy)\n",
        "    \n",
        "# Pass our model_file\n",
        "model_file = './model_cnn_32_64_128/model.ckpt' \n",
        "validate_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_70CkObXdE39",
        "colab_type": "text"
      },
      "source": [
        "## Notes\n",
        "\n",
        "How did the algorithm perform? Did it finish converging? Train a few more thousand iterations, and feel free to change the learning rate and/or batch size as needed to maximize overall algorithm accuracy. What is the final performance on training and test data? \n",
        "\n",
        "If you've tested a few different hyperparametere configurations, you will likely conclude that the algorithm seems to be performing a bit worse on the test data compared to the training data; in other words the algorithm is *overfitting*. Let's go through the following exercises to see what we can do about that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUyXYZlEdJId",
        "colab_type": "text"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "In these exercises, we explore the effect of several strategies used to limit overfitting.\n",
        "\n",
        "## Exercise 1\n",
        "\n",
        "Re-train the same network however this time use batch normalization after the convolutional operation but before the ReLU non-linearity for all layers:\n",
        "\n",
        "```\n",
        "conv > batch-norm > ReLU > ...\n",
        "```\n",
        "\n",
        "Keep in mind that because training mode needs to be specified for batch normalization, we need to create a **new** placeholder. What do you expect to happend to training accuracy? What do you expect to happen with validation accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BXQV4NTq0hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    \"\"\"\n",
        "    Method to create the following CNN architecture:\n",
        "    \n",
        "      - BLOCK 1 (filter depth of 32)\n",
        "      - BLOCK 2 (filter depth of 64)\n",
        "      - RESHAPE\n",
        "      - HIDDEN LAYER (128 nodes)\n",
        "      - LOGIT SCORES (10 nodes)\n",
        "      \n",
        "    \"\"\"\n",
        "    # Reset our graph to build a new one\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define placeholders for our images and labels\n",
        "    # ------------------------------------------------------------------------\n",
        "    #\n",
        "    # 1. Define images\n",
        "    # \n",
        "    #  - type: float32\n",
        "    #  - size: [None, 32, 32, 3] so that we can feed in as many images as we need\n",
        "    # \n",
        "    # 2. Define labels\n",
        "    # \n",
        "    #  - type: int64\n",
        "    #  - size: [None] so that we can feed in as many labels as we need\n",
        "    #\n",
        "    # 3. Define training mode placeholder (for batch normalization)\n",
        "    # \n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    im = tf.placeholder(tf.float32, shape = (None, 32, 32, 3))\n",
        "    labels = tf.placeholder(tf.int64, shape=(None))\n",
        "    training = tf.placeholder(tf.bool)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define convolutional blocks \n",
        "    # ------------------------------------------------------------------------\n",
        "    #\n",
        "    # As in previous assignments, we will use the tf.get_variables(...) method\n",
        "    # to create matrix variables initialized to random values.\n",
        "    # \n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Block 1 (use a filter depth of 32)\n",
        "    w1 = tf.get_variable('w1', shape=[3, 3, 3, 32], dtype=tf.float32)\n",
        "    w2 = tf.get_variable('w2', shape=[3, 3, 32, 32], dtype=tf.float32)\n",
        "    layer = tf.nn.conv2d(im,w1,strides=[1, 1, 1, 1], padding='SAME')\n",
        "    layer = tf.nn.relu(tf.layers.batch_normalization(layer, training=training))\n",
        "    layer = tf.nn.conv2d(layer, w2, strides=[1, 2, 2, 1], padding='SAME')\n",
        "    layer = tf.nn.relu(tf.layers.batch_normalization(layer, training=training))\n",
        "\n",
        "    # Block 2 (use a filter depth of 64)\n",
        "    w3 = tf.get_variable('w3', shape=[3, 3, 32, 64], dtype=tf.float32)\n",
        "    w4 = tf.get_variable('w4', shape=[3, 3, 64, 64], dtype=tf.float32)\n",
        "    layer = tf.nn.conv2d(layer, w3, strides = [1,1,1,1], padding='SAME')\n",
        "    layer = tf.nn.relu(tf.layers.batch_normalization(layer, training=training))\n",
        "    layer = tf.nn.conv2d(layer, w4, strides = [1,2,2,1], padding='SAME')\n",
        "    layer = tf.nn.relu(tf.layers.batch_normalization(layer, training=training))\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Reshape to 1D vector \n",
        "    # ------------------------------------------------------------------------\n",
        "    \n",
        "    flattened = tf.reshape(layer, [-1, 8 * 8 * 64])\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our matmul operations\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Hidden layer (128 nodes)\n",
        "    w5 = tf.get_variable('w5', shape=[8*8*64,128], dtype=tf.float32)\n",
        "    h1 = tf.matmul(flattened, w5)\n",
        "    h1 = tf.nn.relu(tf.layers.batch_normalization(h1, training=training))\n",
        "    \n",
        "    # Logits (10 nodes)\n",
        "    w6 = tf.get_variable('w6', shape=[128,10], dtype=tf.float32)\n",
        "    logits = tf.matmul(h1, w6)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our softmax cross-entropy loss\n",
        "    # ------------------------------------------------------------------------\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our optimizer\n",
        "    # ------------------------------------------------------------------------\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
        "    \n",
        "    return im, labels, training, [w1, w2, w3, w4, w5, w6], logits, loss, train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEHMKch2rJnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Create model \n",
        "# ------------------------------------------------------------------------\n",
        "im, labels, training, weights, logits, loss, train_op = create_model()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Add to collections, initialize graph and saver\n",
        "# ------------------------------------------------------------------------\n",
        "tf.add_to_collection('im', im)\n",
        "tf.add_to_collection('logits', logits)\n",
        "tf.add_to_collection('training', training)\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OMN_7wZVRJ-",
        "colab_type": "text"
      },
      "source": [
        "### Updating train_model() \n",
        "\n",
        "Here we need to define a slightly different `train_model()` algorithm to account for the extra placeholder used to tell the algorithm whether or not batch normalization is in training for validation mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvhbS_5CVKxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Train our algorithm \n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# Let's set up a loop to train our algorithm by feeding it data iteratively.\n",
        "# For each iteration, we will feed a batch_size number of images into our \n",
        "# model and let it readjust it's neuronal weights.\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "def train_model(iterations=1000, batch_size=256):\n",
        "    \n",
        "    accuracies = []\n",
        "    losses = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Grab a total of batch_size number of random images and labels \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        rand_indices = np.random.randint(50000, size=(batch_size))\n",
        "        x_batch = x[rand_indices]\n",
        "        y_batch = y[rand_indices]\n",
        "        y_batch = y_batch[..., 0]\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Normalize x_batch\n",
        "        # --------------------------------------------------------------------\n",
        "        x_batch = (x_batch - np.mean(x_batch)) / np.std(x_batch)\n",
        "\n",
        "        # Convert to types matching our defined placeholders\n",
        "        x_batch = x_batch.astype('float32')\n",
        "        y_batch = y_batch.astype('int64')\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Prepare feed_dict\n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # What extra piece of information (which extra placeholder) do we need\n",
        "        # to fill in when using batch normalization?\n",
        "        #\n",
        "        # --------------------------------------------------------------------\n",
        "        feed_dict = {im: x_batch, labels: y_batch, training: Grue, keep_prob: 0.25}\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Run training iteration via sess.run()\n",
        "        # --------------------------------------------------------------------\n",
        "        outputs = sess.run([logits, loss, train_op], feed_dict)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Use argmax to determine highest logit (model guess)\n",
        "        # --------------------------------------------------------------------\n",
        "        predictions = np.argmax(outputs[0], axis=1)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Calculate accuracy \n",
        "        # --------------------------------------------------------------------\n",
        "        accuracy = np.sum(predictions == y_batch) / batch_size\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Accumulate and print iteration, loss and accuracy \n",
        "        # --------------------------------------------------------------------\n",
        "        print('Iteration %05i | Loss = %07.3f | Accuracy = %0.4f' %\n",
        "            (i + 1, outputs[1], accuracy))\n",
        "\n",
        "        losses.append(outputs[1])\n",
        "        accuracies.append(accuracy)\n",
        "        \n",
        "    return losses, accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG3Y5Cbkse-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------------------------------------------------------\n",
        "# Train model\n",
        "# --------------------------------------------------------------------\n",
        "losses, accuracies = train_model(iterations=3000, batch_size=256)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Graph outputs and accuracy\n",
        "# --------------------------------------------------------------------\n",
        "pylab.plot(losses)\n",
        "pylab.title('Model loss over time')\n",
        "pylab.show()\n",
        "\n",
        "pylab.plot(accuracies)\n",
        "pylab.title('Model accuracy over time')\n",
        "pylab.show()\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Save model\n",
        "# --------------------------------------------------------------------\n",
        "import os\n",
        "model_file = './model_cnn_32_64_128_bn/model.ckpt'\n",
        "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
        "print('Saving model')\n",
        "saver.save(sess, model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlWbFTNMVxkh",
        "colab_type": "text"
      },
      "source": [
        "### Updating validate_model() \n",
        "\n",
        "Here we need to define a slightly different `validate_model()` algorithm to account for the extra placeholder used to tell the algorithm whether or not batch normalization is in training for validation mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROL_VhL0WT_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_model(model_file):\n",
        "    \"\"\"\n",
        "    Method to test the validation performance of a model using the \n",
        "    test set data.\n",
        "    \n",
        "    :params\n",
        "    \n",
        "      (str) model_file : name of model file saved by saver object\n",
        "      \n",
        "    \"\"\"\n",
        "    # Load saved model\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.InteractiveSession()\n",
        "    saver = tf.train.import_meta_graph('%s.meta' % model_file )\n",
        "    saver.restore(sess, model_file)\n",
        "\n",
        "    # Find our placeholders\n",
        "    im = tf.get_collection('im')[0]\n",
        "    logits = tf.get_collection('logits')[0]\n",
        "    training = tf.get_collection('training')[0]\n",
        "\n",
        "    # Normalize our input data x_test\n",
        "    input_data = (x_test - np.mean(x_test)) / np.std(x_test)\n",
        "    \n",
        "    # --------------------------------------------------------------------\n",
        "    # Prepare feed_dict\n",
        "    # --------------------------------------------------------------------\n",
        "    # \n",
        "    # What extra piece of information (which extra placeholder) do we need\n",
        "    # to fill in when using batch normalization?\n",
        "    #\n",
        "    # --------------------------------------------------------------------\n",
        "    feed_dict = {im: input_data, ?}\n",
        "\n",
        "    # Pass data through the network using sess.run() to get our logits \n",
        "    output = sess.run(logits, feed_dict)\n",
        "\n",
        "    # Convert logits to predictions\n",
        "    predictions = np.argmax(output, axis=1)\n",
        "\n",
        "    # Compare predictions to ground-truth to find accuracy\n",
        "    accuracy = np.sum(predictions == y_test[..., 0]) / x_test.shape[0]\n",
        "    \n",
        "    print('Network test-set accuracy: %0.4f' % accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3baMHBPrKBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Validate model \n",
        "# ------------------------------------------------------------------------\n",
        "validate_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DHvUjAxmqzT",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Re-train the same network however this time use 50% dropout in the hidden layer after the  ReLU non-linearity. What do you expect to happend to training accuracy? What do you expect to happen with validation accuracy? What do you expect to happen with speed to convergence?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK63bzQ-tJ_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    \"\"\"\n",
        "    Method to create the following CNN architecture:\n",
        "    \n",
        "      - BLOCK 1 (filter depth of 32)\n",
        "      - BLOCK 2 (filter depth of 64)\n",
        "      - RESHAPE\n",
        "      - HIDDEN LAYER (128 nodes)\n",
        "      - LOGIT SCORES (10 nodes)\n",
        "      \n",
        "    \"\"\"\n",
        "    # Reset our graph to build a new one\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define placeholders for our images and labels\n",
        "    # ------------------------------------------------------------------------\n",
        "    #\n",
        "    # 1. Define images\n",
        "    # \n",
        "    #  - type: float32\n",
        "    #  - size: [None, 32, 32, 3] so that we can feed in as many images as we need\n",
        "    # \n",
        "    # 2. Define labels\n",
        "    # \n",
        "    #  - type: int64\n",
        "    #  - size: [None] so that we can feed in as many labels as we need\n",
        "    # \n",
        "    # 3. Define training mode placeholder (for batch normalization)\n",
        "    # \n",
        "    # 4. Define keep_prob placeholder (for dropout)\n",
        "    # \n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    im = tf.placeholder(tf.float32, shape=?)\n",
        "    labels = tf.placeholder(tf.int64, shape=?)\n",
        "    training = tf.placeholder(tf.bool)\n",
        "    keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define convolutional blocks \n",
        "    # ------------------------------------------------------------------------\n",
        "    #\n",
        "    # As in previous assignments, we will use the tf.get_variables(...) method\n",
        "    # to create matrix variables initialized to random values.\n",
        "    # \n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Block 1 (use a filter depth of 32)\n",
        "    w1 = tf.get_variable('w1', shape=?, dtype=tf.float32)\n",
        "    w2 = tf.get_variable('w2', shape=?, dtype=tf.float32)\n",
        "    layer = tf.nn.conv2d(?)\n",
        "    layer = tf.nn.relu(tf.layers.batch_normalization(layer, training=training))\n",
        "    layer = tf.nn.conv2d(?)\n",
        "    layer = tf.nn.relu(tf.layers.batch_normalization(layer, training=training))\n",
        "\n",
        "    # Block 2 (use a filter depth of 64)\n",
        "    w3 = tf.get_variable('w3', shape=?, dtype=tf.float32)\n",
        "    w4 = tf.get_variable('w4', shape=?, dtype=tf.float32)\n",
        "    layer = tf.nn.conv2d(?)\n",
        "    layer = tf.nn.relu(tf.layers.batch_normalization(layer, training=training))\n",
        "    layer = tf.nn.conv2d(?)\n",
        "    layer = tf.nn.relu(tf.layers.batch_normalization(layer, training=training))\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Reshape to 1D vector \n",
        "    # ------------------------------------------------------------------------\n",
        "    \n",
        "    flattened = tf.reshape(layer, [-1, 8 * 8 * 64])\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our matmul operations\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Hidden layer (128 nodes)\n",
        "    w5 = tf.get_variable('w5', shape=?, dtype=tf.float32)\n",
        "    h1 = tf.matmul(flattened, w5)\n",
        "    h1 = tf.nn.relu(tf.layers.batch_normalization(h1, training=training))\n",
        "    h1 = tf.nn.dropout(?)\n",
        "    \n",
        "    # Logits (10 nodes)\n",
        "    w6 = tf.get_variable('w6', shape=?, dtype=tf.float32)\n",
        "    logits = tf.matmul(h1, w6)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our softmax cross-entropy loss\n",
        "    # ------------------------------------------------------------------------\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our optimizer\n",
        "    # ------------------------------------------------------------------------\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
        "    \n",
        "    return im, labels, training, keep_prob, [w1, w2, w3, w4, w5, w6], logits, loss, train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOTGhzaUtQhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Create model \n",
        "# ------------------------------------------------------------------------\n",
        "im, labels, training, keep_prob, weights, logits, loss, train_op = create_model()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Add to collections, initialize graph and saver\n",
        "# ------------------------------------------------------------------------\n",
        "tf.add_to_collection('im', im)\n",
        "tf.add_to_collection('logits', logits)\n",
        "tf.add_to_collection('training', training)\n",
        "tf.add_to_collection('keep_prob', keep_prob)\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OKiPoKHZfCT",
        "colab_type": "text"
      },
      "source": [
        "### Updating train_model() \n",
        "\n",
        "Here we need to define a slightly different `train_model()` algorithm to account for the extra placeholder used to tell the algorithm what the keep_prob is (50% during training, 100% during validation or inference):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdn7-pfmZfCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Train our algorithm \n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# Let's set up a loop to train our algorithm by feeding it data iteratively.\n",
        "# For each iteration, we will feed a batch_size number of images into our \n",
        "# model and let it readjust it's neuronal weights.\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "def train_model(iterations=1000, batch_size=256):\n",
        "    \n",
        "    accuracies = []\n",
        "    losses = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Grab a total of batch_size number of random images and labels \n",
        "        # --------------------------------------------------------------------\n",
        "        rand_indices = np.random.randint(50000, size=(batch_size))\n",
        "        x_batch = x[rand_indices]\n",
        "        y_batch = y[rand_indices]\n",
        "        y_batch = y_batch[..., 0]\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Normalize x_batch\n",
        "        # --------------------------------------------------------------------\n",
        "        x_batch = (x_batch - np.mean(x_batch)) / np.std(x_batch)\n",
        "\n",
        "        # Convert to types matching our defined placeholders\n",
        "        x_batch = x_batch.astype('float32')\n",
        "        y_batch = y_batch.astype('int64')\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Prepare feed_dict\n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # What extra pieces of information (which extra placeholders) do we need\n",
        "        # to fill in when using batch normalization and dropout?\n",
        "        #\n",
        "        # --------------------------------------------------------------------\n",
        "        feed_dict = {im: x_batch, labels: y_batch, training: True, keep_prob: 0.25}\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Run training iteration via sess.run()\n",
        "        # --------------------------------------------------------------------\n",
        "        outputs = sess.run([logits, loss, train_op], feed_dict)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Use argmax to determine highest logit (model guess)\n",
        "        # --------------------------------------------------------------------\n",
        "        predictions = np.argmax(outputs[0], axis=1)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Calculate accuracy \n",
        "        # --------------------------------------------------------------------\n",
        "        accuracy = np.sum(predictions == y_batch) / batch_size\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Accumulate and print iteration, loss and accuracy \n",
        "        # --------------------------------------------------------------------\n",
        "        print('Iteration %05i | Loss = %07.3f | Accuracy = %0.4f' %\n",
        "            (i + 1, outputs[1], accuracy))\n",
        "\n",
        "        losses.append(outputs[1])\n",
        "        accuracies.append(accuracy)\n",
        "        \n",
        "    return losses, accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypBxACnDtT9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------------------------------------------------------\n",
        "# Train model\n",
        "# --------------------------------------------------------------------\n",
        "losses, accuracies = train_model(iterations=3000, batch_size=256)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Graph outputs and accuracy\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "pylab.plot(losses)\n",
        "pylab.title('Model loss over time')\n",
        "pylab.show()\n",
        "\n",
        "pylab.plot(accuracies)\n",
        "pylab.title('Model accuracy over time')\n",
        "pylab.show()\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Save model\n",
        "# --------------------------------------------------------------------\n",
        "import os\n",
        "model_file = './model_cnn_32_64_128_bn_drop/model.ckpt'\n",
        "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
        "print('Saving model')\n",
        "saver.save(sess, model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYSiOVvlZ6rM",
        "colab_type": "text"
      },
      "source": [
        "### Updating validate_model() \n",
        "\n",
        "Here we need to define a slightly different `validate_model()` algorithm to account for the extra placeholder used to tell the algorithm what the keep_prob should be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrUuatqQZ6rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_model(model_file):\n",
        "    \"\"\"\n",
        "    Method to test the validation performance of a model using the \n",
        "    test set data.\n",
        "    \n",
        "    :params\n",
        "    \n",
        "      (str) model_file : name of model file saved by saver object\n",
        "      \n",
        "    \"\"\"\n",
        "    # Load saved model\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.InteractiveSession()\n",
        "    saver = tf.train.import_meta_graph('%s.meta' % model_file )\n",
        "    saver.restore(sess, model_file)\n",
        "\n",
        "    # Find our placeholders\n",
        "    im = tf.get_collection('im')[0]\n",
        "    logits = tf.get_collection('logits')[0]\n",
        "    training = tf.get_collection('training')[0]\n",
        "    keep_prob = tf.get_collection('keep_prob')[0]\n",
        "\n",
        "    # Normalize our input data x_test\n",
        "    input_data = (x_test - np.mean(x_test)) / np.std(x_test)\n",
        "    \n",
        "    # --------------------------------------------------------------------\n",
        "    # Prepare feed_dict\n",
        "    # --------------------------------------------------------------------\n",
        "    # \n",
        "    # What extra pieces of information (which extra placeholders) do we need\n",
        "    # to fill in when using batch normalization and dropout?\n",
        "    #\n",
        "    # --------------------------------------------------------------------\n",
        "    feed_dict = {im: x_batch, labels: y_batch, training: False, keep_prob: 1}\n",
        "\n",
        "    # Pass data through the network using sess.run() to get our logits \n",
        "    output = sess.run(logits, feed_dict)\n",
        "\n",
        "    # Convert logits to predictions\n",
        "    predictions = np.argmax(output, axis=1)\n",
        "\n",
        "    # Compare predictions to ground-truth to find accuracy\n",
        "    accuracy = np.sum(predictions == y_test[..., 0]) / x_test.shape[0]\n",
        "    \n",
        "    print('Network test-set accuracy: %0.4f' % accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL2Tek4UtW8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Validate model \n",
        "# ------------------------------------------------------------------------\n",
        "validate_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vxvdw7hnHaE",
        "colab_type": "text"
      },
      "source": [
        "# Advanced Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FGx9e1KnP_f",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Add L2 regularization to the model. Keep in the mind the following necessary step:\n",
        "\n",
        "* add together the summed (`tf.reduce_sum()`) squared version of all the weights\n",
        "* multiple this value by some constant\n",
        "* combine L2 regularization loss + softmax cross-entropy loss\n",
        "* pass the combined loss to the optimizer `minimize()` function\n",
        "\n",
        "Try training different models with different L2 regularization. What do you expect to happen to training accuracy? Validation accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXS23WlQqqH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxdquaijn3R9",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "One additional very useful strategy to prevent overfitting is network architecture design. Which components of the currecnt CNN architecture could be improved in this regard? \n",
        "\n",
        "Some thoughts:\n",
        "\n",
        "* add extra convolutional layers to decrease the feature map to 4 x 4 x N (instead of 8 x 8) before reshaping and apply matrix multiplications\n",
        "* decrease the size of the hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP-I3hD7qpqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}