{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03 - Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uMVMHwL9FCz",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "\n",
        "In the previous assignment you learned the mechanics of creating a basic, single-layer linear classifier. In this next notebook, we will walk through the steps needed to add additional layers.\n",
        "\n",
        "Recall that in addition to simply adding more layers to the network, the most important new technique is the use of **non-linearities**. The use of non-linear functions prevents the collapse (or reduction) of a series of matrix operations into a single step (\"reflex\") loop. \n",
        "\n",
        "Additionally, as you begin to create and test more models, it will be important to create a system for organizing your code and maintainng changes. Here we will re-apply some of the major concepts in object-oriented programming to help facilitate keep your experiments on track."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDDCr1KRFw-d",
        "colab_type": "text"
      },
      "source": [
        "## Restarting your Virtual Machine\n",
        "\n",
        "If at any point during this assignment you accidentally execute code or do something that cannot seem to undo and need to \"restart\" the system (including deleting all temporary folders), go ahead and run the following single line of code. It will take about 1 minute to restart. Following this, you will have to proceed at the beginning of the assignment to re-downloaded the data and run the code you have written. Note, the code that you have already written will **not** be deleted; you simply need to start executing the code once again from the start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyP9zHBGF4kv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1 # Warning this restarts your machine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etbdY9hqF8Iv",
        "colab_type": "text"
      },
      "source": [
        "## Downloading the Data\n",
        "\n",
        "The following commands can be used to copy over the assignment materials to your local Colaboratory instance and unzip in preparation for your assignment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IybSQkWLF_oT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/CAIDMRes/lecture_02\n",
        "!unzip lecture_02/data.zip\n",
        "!rm -r lecture_02\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EMSccZqGDto",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP52YS5UGGhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading a pickle (*.pkl) file\n",
        "import pickle\n",
        "x = pickle.load(open('x.pkl', 'rb'))\n",
        "\n",
        "# x is a NumPy array with (flattened) image data\n",
        "print(type(x))\n",
        "print(x.shape)\n",
        "\n",
        "# y is a NumPy array with labels\n",
        "y = pickle.load(open('y.pkl', 'rb'))\n",
        "print(y.shape) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo6W2GpQ_l6B",
        "colab_type": "text"
      },
      "source": [
        "# Non-linear Functions\n",
        "\n",
        "The interposition of *any* non-linear function can serve as a basis for neural network design. Currently the most common non-linear function used in modern neural networks is known as the **ReLU** (rectified linear unit), defined as:\n",
        "\n",
        "```\n",
        "relu = max(0, x)\n",
        "```\n",
        "![relu](https://github.com/CAIDMRes/images/blob/master/relu.jpg?raw=true)\n",
        "\n",
        "Due to some favorable features related to matrix calculus (e.g. the slope is constant) and ease of calculation / implementation, it is a quite widely accepted function to use.\n",
        "\n",
        "Tensorflow as has a very simple implementation of the ReLU function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POczFtIEBkXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "A = np.random.normal(loc=0, scale=1, size=(10))\n",
        "a = tf.placeholder(tf.float32, [10])\n",
        "b = tf.nn.relu(a)\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "output = sess.run(b, feed_dict={a: A})\n",
        "\n",
        "print(A)\n",
        "print(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgi2sRcqCYIt",
        "colab_type": "text"
      },
      "source": [
        "# Multilayer Network\n",
        "\n",
        "Let's go ahead an incorporate the use of ReLU non-linearities in our first multiple layer (true) neural network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI-QTcbgCnlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_hidden_layer_1 = 128 \n",
        "\n",
        "def create_model():\n",
        "    \n",
        "    # Reset our graph to build a new one\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define placeholders for our images and labels\n",
        "    # ------------------------------------------------------------------------\n",
        "    #\n",
        "    # 1. Define images\n",
        "    # \n",
        "    #  - type: float32\n",
        "    #  - size: [None, 784] so that we can feed in as many images as we need\n",
        "    # \n",
        "    # 2. Define labels\n",
        "    # \n",
        "    #  - type: int64\n",
        "    #  - size: [None] so that we can feed in as many labels as we need\n",
        "    # \n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    im = tf.placeholder(?)\n",
        "    labels = tf.placeholder(?)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our weights\n",
        "    # ------------------------------------------------------------------------\n",
        "    # \n",
        "    # Let us use a new variable initializer, the tf.get_variables(...) method.\n",
        "    # In this method call, we simply need to provide the function with the \n",
        "    # variable name, size, and type. Tensorflow will subsequently use this \n",
        "    # information to generate a reasonable starting distribution.\n",
        "    # \n",
        "    # Keep in mind that a different set of weights (connections) must be \n",
        "    # created for each separate layer in our graph. Make sure that the matrix\n",
        "    # dimensions match up.\n",
        "    # \n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    w1 = tf.get_variable('w1', ?, dtype=tf.float32)\n",
        "    w2 = tf.get_variable('w2', ?, dtype=tf.float32)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our matmul operations\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    h1 = tf.nn.relu(tf.matmul(?))\n",
        "    logits = tf.matmul(?)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our softmax cross-entropy loss\n",
        "    # ------------------------------------------------------------------------\n",
        "    # \n",
        "    # HINT: use tf.losses.sparse_softmax_cross_entropy() as above\n",
        "    #\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "    \n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our optimizer\n",
        "    # ------------------------------------------------------------------------\n",
        "    # \n",
        "    # An optimizer is a special TensorFlow class that takes your model weights and \n",
        "    # adjusts them ever so slightly so that they will make a better prediction the\n",
        "    # next time around. They are implemented with a technique known as \n",
        "    # backpropogation which we will learn about in further detail during later \n",
        "    # lectures. For now, just know that this is what we are using here.\n",
        "    #\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
        "    \n",
        "    return im, labels, [w1, w2], logits, loss, train_op\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Test our model\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# If the graph was defined properly, we should be able to check the out\n",
        "# what the model outputs should look like. Can you guess by the shapes\n",
        "# of our logits and losses will be?\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "im, labels, weights, logits, loss, train_op = create_model()\n",
        "print(logits.shape)\n",
        "print(loss.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqjmVk5AEZF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Create our model\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "im, labels, weights, logits, loss, train_op = create_model()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Add to collections\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# Collections are used by TensorFlow to keep track of certain intermediate \n",
        "# values for quick access during save/load functions.\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "tf.add_to_collection('im', im)\n",
        "tf.add_to_collection('logits', logits)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Initialize our test graph\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# What two things do we need to initialize our graph?\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "sess = ?\n",
        "tf.?\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Initialize our test graph\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# Initialize a Saver object\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCCjZTumGrph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Train our algorithm \n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# Let's set up a loop to train our algorithm by feeding it data iteratively.\n",
        "# For each iteration, we will feed a batch_size number of images into our \n",
        "# model and let it readjust it's neuronal weights.\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "def train_model(iterations=2000, batch_size=256):\n",
        "    \n",
        "    accuracies = []\n",
        "    losses = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Grab a total of batch_size number of random images and labels \n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # 1. Pick batch_size number of random indices between 0 and 60,000\n",
        "        # 2. Select those images / labels\n",
        "        #\n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        rand_indices = np.random.randint(?)\n",
        "        x_batch = x[?]\n",
        "        y_batch = y[?]\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Normalize x_batch\n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # Currently, values in x range from 0 to 255. If we normalize these values\n",
        "        # to a mean of 0 and SD of 1 we will improve the stability of training\n",
        "        # and furthermore improve interpretation of learned weights. Use the\n",
        "        # following code to normalize your batch:\n",
        "        # \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        x_batch = (x_batch - np.mean(x_batch)) / np.std(x_batch)\n",
        "\n",
        "        # Convert to types matching our defined placeholders\n",
        "        x_batch = x_batch.astype(?)\n",
        "        y_batch = y_batch.astype(?)\n",
        "\n",
        "        # Prepare feed_dict\n",
        "        feed_dict = {?}\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Run training iteration via sess.run()\n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # Here, in addition to whichever ouputs we wish to extract, we need to\n",
        "        # also include the train_op variable. Including train_op will tell \n",
        "        # Tensorflow that in addition to calculating the intermediates of our graph,\n",
        "        # we also need to readjust the variables so that the overall loss goes\n",
        "        # down.\n",
        "        # \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        outputs = sess.run([logits, loss, train_op], feed_dict)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Use argmax to determine highest logit (model guess)\n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # Keep in mind our logits matrix is (batch_size x 10) in size representing\n",
        "        # a total of batch_size number of predictions. How do we process this matrix\n",
        "        # with the np.argmax() to find the highest logit along each row of the matrix\n",
        "        # (e.g. find the prediction for each of our images)?\n",
        "        # \n",
        "        # HINT: what does the axis parameter in np.argmax(a, axis) specify?\n",
        "        # \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        predictions = np.argmax(?)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Calculate accuracy \n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # Consider the following:\n",
        "        # \n",
        "        # - predictions = the predicted digits\n",
        "        # - y_batch = the ground-truth digits\n",
        "        # \n",
        "        # How do I calculate an accuracy % with this data?\n",
        "        # \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        accuracy = np.sum(?) / ?\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Accumulate and print iteration, loss and accuracy \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        print('Iteration %05i | Loss = %07.3f | Accuracy = %0.4f' %\n",
        "            (i + 1, outputs[1], accuracy))\n",
        "\n",
        "        losses.append(outputs[1])\n",
        "        accuracies.append(accuracy)\n",
        "        \n",
        "    # --------------------------------------------------------------------\n",
        "    # Graph outputs and accuracy\n",
        "    # --------------------------------------------------------------------\n",
        "\n",
        "    import pylab\n",
        "    pylab.plot(losses)\n",
        "    pylab.title('Model loss over time')\n",
        "    pylab.show()\n",
        "\n",
        "    pylab.plot(accuracies)\n",
        "    pylab.title('Model accuracy over time')\n",
        "\n",
        "    return losses, accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXIEmymoDcKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------------------------------------------------------\n",
        "# Train model\n",
        "# --------------------------------------------------------------------\n",
        "losses, accuracies = train_model(iterations=2000, batch_size=256)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Save model\n",
        "# --------------------------------------------------------------------\n",
        "# \n",
        "# In this step, all model variables and the underlying graph structure\n",
        "# are saved so that they can be reloaded. Although it looks like just one\n",
        "# file is saved here, in fact both a *.cpkt and *.cpkt.meta file are both\n",
        "# saved in this single line of code.\n",
        "#  \n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "model_file = './model_128/model.ckpt'\n",
        "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
        "print('Saving model')\n",
        "saver.save(sess, model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbnSxArTEiOu",
        "colab_type": "text"
      },
      "source": [
        "Congratulations! If that went well, you have now trained your first single-layer neural network. The accuracy seems quite impressive, much better than the linear classifier... have we solved this problem yet? Well let's go ahead and perform some checks to see just how good we're doing.\n",
        "\n",
        "## Model weights\n",
        "\n",
        "Compared to the average filter linear classifier, where the weights have a very obvious and intuitive meaning, these model filters are much more complex. What the algorithm has learned is to identify 128 different filters, which when used in combination with each other, can predict the likelihood of a digit. Therefore, each one of these filters captures a small \"component\" of any given digit. Let's take a look"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eUM-_cCEqIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------------------------------------------------------\n",
        "# Extract model weights \n",
        "# --------------------------------------------------------------------\n",
        "# \n",
        "# What line of code do we need here to extract the model weights?\n",
        "# \n",
        "# HINT: what do we pass to sess.run()?\n",
        "#\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "W = sess.run(?)\n",
        "\n",
        "# Visualize\n",
        "import pylab\n",
        "fig = pylab.figure()\n",
        "for i in range(24):\n",
        "    fig.add_subplot(4, 6, i + 1)\n",
        "    pylab.axis('off')\n",
        "    pylab.imshow(W[..., i].reshape(28, 28))\n",
        "    \n",
        "pylab.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DVlUqJiEvsv",
        "colab_type": "text"
      },
      "source": [
        "# Running inference\n",
        "\n",
        "Now that we have a trained model, let's go ahead and see how it performs! We will use the same procedure as before to load up a trained network and then feed in random digits to see how it fares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_89Ock9hFMZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the saved model\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "saver = tf.train.import_meta_graph('./model_128/model.ckpt.meta')\n",
        "saver.restore(sess, './model_128/model.ckpt')\n",
        "\n",
        "# Find our placeholders\n",
        "im = tf.get_collection('im')[0]\n",
        "logits = tf.get_collection('logits')[0]\n",
        "\n",
        "# Find a random test image\n",
        "i = int(?)\n",
        "image = x[i].reshape(1, 784)\n",
        "label = y[i]\n",
        "\n",
        "# Normalize the image\n",
        "image = (image - np.mean(image)) / np.std(image)\n",
        "\n",
        "# Create a feed_dict\n",
        "feed_dict = {?}\n",
        "\n",
        "# Pass data through the network\n",
        "l = sess.run(logits, ?)\n",
        "\n",
        "# Convert logits to predictions\n",
        "prediction = np.argmax(?)\n",
        "\n",
        "# Visualize\n",
        "pylab.imshow(image.reshape(28, 28))\n",
        "pylab.axis('off')\n",
        "pylab.title('My prediction is %i' % prediction)\n",
        "pylab.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-cBnpB8M6HP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls ./model_128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHUQk6yN2v-Y",
        "colab_type": "text"
      },
      "source": [
        "# Model validation\n",
        "\n",
        "As we learned about in lecture #4, a model with *too much* learning capacity can potentially memorize the dataset without learning anything too useful. The fact that we trained to an accuracy so close to 100% means that we should be wary that the algorithm may be at least in part memorizing portions of the dataset. To test for this phenomenon we need to evaluate the model on new data that the algorithm has never seen before. Let's go ahead download this new data now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha3OXFfk4Me0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/CAIDMRes/lecture_03\n",
        "!unzip lecture_03/data.zip\n",
        "!rm -r lecture_03 \n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gqMMgoz4h-A",
        "colab_type": "text"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "The two new files we downloaded are `x_test.npy` and `y_test.npy` corresponding to our test set data and labels.  The format is identical to before. We have a total of 10,000 examples to test. Let us load them now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ujL2Fyw5CC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "x_test = np.load('x_test.npy',allow_pickle=True)\n",
        "y_test = np.load('y_test.npy',allow_pickle=True)\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUr6KfWm5VzR",
        "colab_type": "text"
      },
      "source": [
        "## Validating\n",
        "\n",
        "Using the template code to run inference shown above, let us now write code to:\n",
        "\n",
        "* load our saved model \n",
        "* create a `feed_dict` with new test data \n",
        "* pass through network using `sess.run()`\n",
        "* convert the `logits` to predictions\n",
        "* calculate overall network accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtE3y7ab6VUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_model(model_file):\n",
        "    \"\"\"\n",
        "    Method to test the validation performance of a model using the \n",
        "    test set data.\n",
        "    \n",
        "    :params\n",
        "    \n",
        "      (str) model_file : name of model file saved by saver object\n",
        "      \n",
        "    \"\"\"\n",
        "    # Load saved model\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.InteractiveSession()\n",
        "    saver = tf.train.import_meta_graph('%s.meta' % model_file)\n",
        "    saver.restore(sess, model_file)\n",
        "\n",
        "    # Find our placeholders\n",
        "    im = tf.get_collection('im')[0]\n",
        "    logits = tf.get_collection('logits')[0]\n",
        "\n",
        "    # Normalize our input data x_test\n",
        "    input_data = (x_test - np.mean(x_test, axis=1, keepdims=True)) / \\\n",
        "        np.std(x_test, axis=1, keepdims=True)\n",
        "    \n",
        "    # -------------------------------------------------------\n",
        "    # Create a feed_dict\n",
        "    # -------------------------------------------------------\n",
        "    # \n",
        "    # HINT: What type of data can we possibly feed into our network?\n",
        "    # What happens if we feed in more than one image at a time?\n",
        "    # Think about the rules for matrix multiplication.\n",
        "    # \n",
        "    # -------------------------------------------------------\n",
        "\n",
        "    feed_dict = {?}\n",
        "\n",
        "    # Pass data through the network using sess.run() to get our logits \n",
        "    output = sess.run(logits, ?)\n",
        "\n",
        "    # Convert logits to predictions\n",
        "    predictions = np.argmax(?)\n",
        "\n",
        "    # Compare predictions to ground-truth to find accuracy\n",
        "    accuracy = np.sum(?) / ?\n",
        "    \n",
        "    print('Network test-set accuracy: %0.4f' % accuracy)\n",
        "    \n",
        "# Pass our model_file\n",
        "model_file = './model_128/model.ckpt' \n",
        "validate_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vOa7h-M7ZSj",
        "colab_type": "text"
      },
      "source": [
        "## Notes\n",
        "\n",
        "How did the algorithm perform? Were you surprised, not surprised? Regardless it's still quite a bit better than our linear classier which maxed out around 90-92%. In the remainder of this tutorial, let's test a handful of different architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPsH75X48Vd5",
        "colab_type": "text"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "For the following exercises, we will evaluate a number of different variations in network architecture. Generally, the steps will include:\n",
        "\n",
        "* writing a new model in the `create_model()` method\n",
        "* initial training variables\n",
        "* use the `train_model()` method defined above to run training (repeated as many times needed to converge)\n",
        "* save model\n",
        "* validate model on test set data\n",
        "\n",
        "The goal is to get a sense of which combinations work better than others. Keep in mind we are already at high 97%+ accuracy, so we're not expecting any dramatic changes, but the process fine-tuning a neural network is an extremely valuable experience to gain first-hand.\n",
        "\n",
        "## Exercise 1\n",
        "\n",
        "Re-train several neural networks this time with either more (196, 256) or less (96, 64, 32) nodes. What do you expect to happen to your algorithm accuracy? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-0rrsdTEf33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_hidden_layer_1 = 256\n",
        "\n",
        "def create_model():\n",
        "    \n",
        "    # Reset our graph to build a new one\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define placeholders for our images and labels\n",
        "    # ------------------------------------------------------------------------\n",
        "    im = tf.placeholder(tf.float32, [None, 784])\n",
        "    labels = tf.placeholder(tf.int64,[None])\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our weights\n",
        "    # ------------------------------------------------------------------------\n",
        "    w1 = tf.get_variable('w1', [784, ?], dtype=tf.float32)\n",
        "    w2 = tf.get_variable('w2', [?, 10], dtype=tf.float32)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our matmul operations\n",
        "    # ------------------------------------------------------------------------\n",
        "    h1 = tf.nn.relu(tf.matmul(?))\n",
        "    logits = tf.matmul(?)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our softmax cross-entropy loss\n",
        "    # ------------------------------------------------------------------------\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our optimizer\n",
        "    # ------------------------------------------------------------------------\n",
        "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
        "    \n",
        "    return im, labels, [w1, w2], logits, loss, train_op\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Create model \n",
        "# ------------------------------------------------------------------------\n",
        "im, labels, weights, logits, loss, train_op = create_model()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Add to collections, initialize graph and saver\n",
        "# ------------------------------------------------------------------------\n",
        "tf.add_to_collection('im', im)\n",
        "tf.add_to_collection('logits', logits)\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgKBcygRpNnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Train model \n",
        "# ------------------------------------------------------------------------\n",
        "losses, accuracies = train_model(iterations=2000, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDiQ25OpslpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Save your model\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# HINT: use a meaningful naming convention so you remember which model is \n",
        "# what during validation\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "model_file = './model_128/model.test'\n",
        "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
        "print('Saving model')\n",
        "saver.save(sess, model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJSeFyGhpnMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Validate model \n",
        "# ------------------------------------------------------------------------\n",
        "validate_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U22DQ5dVEgg2",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Re-train several new networks, however this time use **two** hidden layers (instead of one). Try experimenting with large number intermediate nodes (e.g. 128, 128) or low number intermediate nodes (32, 32), or some combination of both. Were you able to get a better result than the single hidden layer model. Why or why not? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xXEI5M69_9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "n_hidden_layer_1 = 128\n",
        "n_hidden_layer_2 = 64\n",
        "n_hidden_layer_3 = 32\n",
        "n_hidden_layer_4 = 16\n",
        "\n",
        "def create_model():\n",
        "    \n",
        "    # Reset our graph to build a new one\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define placeholders for our images and labels\n",
        "    # ------------------------------------------------------------------------\n",
        "    im = tf.placeholder(tf.float32,[None, 784])\n",
        "    labels = tf.placeholder(tf.int64,[None])\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our weights\n",
        "    # ------------------------------------------------------------------------\n",
        "    w1 = tf.get_variable('w1', [784, ?],dtype=tf.float32)\n",
        "    w2 = tf.get_variable('w2', [?, ?], dtype=tf.float32)\n",
        "    w3 = tf.get_variable('w3', [?, ?], dtype=tf.float32)\n",
        "    w4 = tf.get_variable('w4', [?, ?], dtype=tf.float32)\n",
        "    w5 = tf.get_variable('w5', [?, 10], dtype=tf.float32)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our matmul operations\n",
        "    # ------------------------------------------------------------------------\n",
        "    h1 = tf.nn.relu(tf.matmul(?))\n",
        "    h2 = tf.nn.relu(tf.matmul(?))\n",
        "    h3 = tf.nn.relu(tf.matmul(?))\n",
        "    h4 = tf.nn.relu(tf.matmul(?))\n",
        "    logits = tf.matmul(?)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our softmax cross-entropy loss\n",
        "    # ------------------------------------------------------------------------\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our optimizer\n",
        "    # ------------------------------------------------------------------------\n",
        "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
        "    \n",
        "    return im, labels, [w1, w2, w3], logits, loss, train_op\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Create model \n",
        "# ------------------------------------------------------------------------\n",
        "im, labels, weights, logits, loss, train_op = create_model()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Add to collections, initialize graph and saver\n",
        "# ------------------------------------------------------------------------\n",
        "tf.add_to_collection('im', im)\n",
        "tf.add_to_collection('logits', logits)\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hme1Grru-AnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Train model \n",
        "# ------------------------------------------------------------------------\n",
        "losses, accuracies = train_model(iterations=2000, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QTm9-dQEtaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Save your model\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# HINT: use a meaningful naming convention so you remember which model is \n",
        "# what during validation\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "model_file = './model_64/model.cpkt'\n",
        "os.makedirs(model_file, exist_ok=True)\n",
        "print('Saving model')\n",
        "saver.save(sess, model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJBJgCScBYkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Validate model \n",
        "# ------------------------------------------------------------------------\n",
        "validate_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xcgNyY1rSZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}