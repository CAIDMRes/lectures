{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04 - Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfUIPsyfU43B",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "\n",
        "So far in the first three assignments you have learned the evolution of AI algorithms from expert (rule-based) systems, linear classifiers and regular artificial neural networks (ANNs). To finish off the series, we will finally implement a **convolutional** neural network (CNN), a special type of neural network that is customized specifically for imaging data and which is now widely recognized as the state-of-the-art for nearly all image recognition tasks.\n",
        "\n",
        "## Outline\n",
        "\n",
        "* convolutional operations\n",
        "* non-linearities\n",
        "* spatial sub-sampling\n",
        "* fully-connected layers\n",
        "* training a CNN\n",
        "* inference\n",
        "* validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efi-AKsFXGR7",
        "colab_type": "text"
      },
      "source": [
        "## Restarting your Virtual Machine\n",
        "â€‹\n",
        "If at any point during this assignment you accidentally execute code or do something that cannot seem to undo and need to \"restart\" the system (including deleting all temporary folders), go ahead and run the following single line of code. It will take about 1 minute to restart. Following this, you will have to proceed at the beginning of the assignment to re-downloaded the data and run the code you have written. Note, the code that you have already written will **not** be deleted; you simply need to start executing the code once again from the start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feYxnR_yXIJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1 # Warning this restarts your machine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdhVLlM0XLEK",
        "colab_type": "text"
      },
      "source": [
        "## Downloading the Data\n",
        "\n",
        "The following commands can be used to copy over the assignment materials to your local Colaboratory instance and unzip in preparation for your assignment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TypLEulDXOmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/CAIDMRes/lecture_02\n",
        "!unzip lecture_02/data.zip\n",
        "!rm -r lecture_02\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANKQf7efXUye",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sApZIwCNXW_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading a pickle (*.pkl) file\n",
        "import pickle\n",
        "x = pickle.load(open('x.pkl', 'rb'))\n",
        "\n",
        "# x is a NumPy array with (flattened) image data\n",
        "print(type(x))\n",
        "print(x.shape)\n",
        "\n",
        "# y is a NumPy array with labels\n",
        "y = pickle.load(open('y.pkl', 'rb'))\n",
        "print(y.shape) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLb6jC5jXafv",
        "colab_type": "text"
      },
      "source": [
        "# Convolutions\n",
        "\n",
        "The primary difference between a regular ANN vs. CNN is the use of *convolutional* filters (kernels). Compared to global filters used in a regular ANN, CNNs use local convolutional filters which act uniformly at many different regions in the entire image. Whereas an ANN tests for the presence of a relatively *dense* (high-resolution) pattern throughout the image resulting in a single activation value per node, a CNN tests for the presence of a relatively *small* (low-resolution) pattern at many different image subregions resulting in a **feature map** per node. \n",
        "\n",
        "![Convolutions](https://raw.githubusercontent.com/CAIDMRes/images/master/assignment_05-01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbIR7IIHaYzY",
        "colab_type": "text"
      },
      "source": [
        "## What are convolutions?\n",
        "\n",
        "Convolutions are a mathematic construct to multiply a given smaller matrix (filter or kernel) at various locations within a larger matrix. The intuitive explanation is that a convolution checks for the how well a particular subregion of the matrix matches the target filter or kernel. If there is a strong match, the overall output value (multiplication followed by summation) is high. \n",
        "\n",
        "In the following figure, the original 4 x 4 matrix (blue) is between convolved with a 3 x 3 kernel (dark blue) with the result of the operation stored in the top-left hand corner of the output matrix (dark green). \n",
        "\n",
        "![Convolution](https://raw.githubusercontent.com/CAIDMRes/images/master/assignment_05-02.png)\n",
        "\n",
        "For an excellent, highly-recommended (visual) refresher for convolutional operations, see the following [link](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html).\n",
        "\n",
        "##  Convolutional Math\n",
        "\n",
        "Recall that there are three main parameters that you must define with convolutional operations. These include:\n",
        "\n",
        "### Filter size\n",
        "\n",
        "Recall that convolution of 2D images requires the use of a 4D convolutional kernel of size `i x j x c0 x c1`. This kernel size has two main components:\n",
        "\n",
        "* `i x j`: this is the 2D size of filter (convolutional kernel) you want to apply to your original matrix; common sizes here include 3 x 3, 5 x 5 or 7 x 7.\n",
        "* `c0 x c1`: this is size of the input image depth (`c0`) and the output image depth (`c1`); recall that our original greyscale images are the equivalent of a single image with depth of 1, whereas for every other layer in our CNN this depth corresponds to the number of feature maps in a given layer\n",
        "\n",
        "### Stride\n",
        " \n",
        "The stride defines whether to apply to filter to every location (stride 1), every other location (stride 2), etc. Recall that strides > 1 will result in an output matrix that is smaller by a factor proportional to the stride (e.g. stride 2 results in an output matrix that is 1/2th the original size).\n",
        "\n",
        "### Padding\n",
        "\n",
        "The padding strategy determines whether or not to pad the boundary of the image with 0s (or some other value) prior to convolutions. Recall that a **same** strategy of padding simply pads the image with enough 0s such that the output matrix is the *same size* as the input matrix, compared to a **valid** strategy of padding which does *not* pad the image at all and insteads only performs convolutions on the *valid* portion of the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkeJp4aSbo-I",
        "colab_type": "text"
      },
      "source": [
        "## Convolutions with Tensorflow\n",
        "\n",
        "Now that we understand the key parameters needed to define a convolution operation, implemention is Tensorflow is quite simple. The key consideration is understanding the four different dimensions of our matrices, `N x H x W x C`. To begin, `H x W` simply corresponds to the height and width of our image or feature maps (for our 784-element MNIST digits `H x W` = 28 x 28). Next,  `C` represents the depth (number of \"channels\") of our image or feature maps (for MNIST digits `C` = 1 but for every other intermediate feature map it represents the total number of maps that are stacked together). Finally, `N` as in our previous assignments represents the number of images we pass into the network for now we will leave undefined (`None`) so that the network can be flexible. \n",
        "\n",
        "This standard notation for variable matrix size is used throughout Tensorflow and other deep learning libraries so it is useful to commit to memory. In addition to the images and feature maps, the same 4D convention is used to define operation stride. For all except the most specialized use cases, stride will be equal to 1 along both the `N` and `C` dimensions, with variations in stride primarily defined along the `H x W` direction (e.g to apply an operation that strides by 2 along the height and width directions would require a stride length = `[1, 2, 2, 1]`).\n",
        "\n",
        "Let's take a look at all this here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnQj3MMZka3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# (1) Define a placeholder for the original image\n",
        "im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "\n",
        "# (2) Define a placeholder for the first convolutional kernel\n",
        "w1 = tf.placeholder(tf.float32, [5, 5, 1, 16])\n",
        "\n",
        "# (3) Apply a convolution\n",
        "output = tf.nn.conv2d(im, w1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "# What is the size of our output matrix?\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEYBBJL-n1lW",
        "colab_type": "text"
      },
      "source": [
        "Does the size of our output matrix make sense? Keep in mind that in this particular example with a `5 x 5 x 1 x 16` convolutional kernel, we are creating a \"bank\" of 16 different filters each of size `5 x 5` then applying to our input matrix (which has a depth of 1). Accordingly, this operation results in an output of 16 total feature maps, each of size `28 x 28`.\n",
        "\n",
        "Let's see if you can solve through the next additional examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnatWss9omtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ==============================================================================\n",
        "# Exercise 1 \n",
        "# ==============================================================================\n",
        "# \n",
        "# Using the same inputs, define an operation with a stride of length of 2 in both\n",
        "# the height (H) and width (W) directions (but 1 in all remaining directions).\n",
        "# What is the output shape?\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "w1 = tf.placeholder(tf.float32, [5, 5, 1, 16])\n",
        "output = tf.nn.conv2d(?) \n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpOmIs5xpxFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ==============================================================================\n",
        "# Exercise 2 \n",
        "# ==============================================================================\n",
        "# \n",
        "# Using the same inputs, define an operation with a padding operation of 'VALID'\n",
        "# instead of `SAME` (stride length of 1). What is the output shape?\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "w1 = tf.placeholder(tf.float32, [5, 5, 1, 16])\n",
        "output = tf.nn.conv2d(?) \n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MoxNRNWqNBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ==============================================================================\n",
        "# Exercise 3 \n",
        "# ==============================================================================\n",
        "# \n",
        "# Using the same inputs, define an operation with a filter size of 7 x 7 instead\n",
        "# of 5 x 5 ('SAME' padding, stride length of 1) What is the output shape?\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "w1 = tf.placeholder(tf.float32, [7, 7, 1, 16])\n",
        "output = tf.nn.conv2d(?) \n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbSZ5fAcrSUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ==============================================================================\n",
        "# Exercise 4 \n",
        "# ==============================================================================\n",
        "# \n",
        "# Create a new input that has an image depth of 4 instead of 1. In medical \n",
        "# imaging, these scenario is common when we would like to use different MR  \n",
        "# series (T1, T2, etc) of the same body all at the same time to make an \n",
        "# algorithm. Assuming a 5 x 5 convolutional kernel with 16 output maps, what is\n",
        "# the full convolutional kernel size required for this operation?\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "im = tf.placeholder(tf.float32, [None, 28, 28, 4])\n",
        "w1 = tf.placeholder(tf.float32, ?) # What size is needed here?\n",
        "output = tf.nn.conv2d(im, w1, strides=[1, 1, 1, 1], padding='SAME') \n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdvxGkK3qc5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ==============================================================================\n",
        "# **ADVANCED** \n",
        "# ==============================================================================\n",
        "# \n",
        "# (1) Define an operation with asymmetric striding (1 along H, 2 along W). What\n",
        "#     is the resulting output shape?\n",
        "# \n",
        "# (2) Define an operation with `VALID` padding and stride 2 along H and W. What\n",
        "#     is the resulting output shape? What is the formula for resulting output \n",
        "#     shape when both `VALID` padding strategy and stride > 1 are used?\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "w1 = tf.placeholder(tf.float32, [5, 5, 1, 16])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKJNHqg5bsyU",
        "colab_type": "text"
      },
      "source": [
        "# Non-linearities\n",
        "\n",
        "As in regular (non-convolutional) neural networks, the same ReLU nonlinearity function is used for CNNs. Just like the ReLU is applied after a matrix multiply operation in ANNs, a ReLU is applied after a convolutional operation in CNNs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD71wn8MsHtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define inputs\n",
        "im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "w1 = tf.placeholder(tf.float32, [5, 5, 1, 16])\n",
        "\n",
        "# Perform convolution + ReLU\n",
        "output = tf.nn.relu(tf.nn.conv2d(im, w1, strides=[1, 1, 1, 1], padding='SAME'))\n",
        "\n",
        "# What is the size of the output matrix?\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L5w9NS3bu3g",
        "colab_type": "text"
      },
      "source": [
        "# Spatial Downsampling\n",
        "\n",
        "As we progress through a series of convolutional operations, each resulting new stack of feature maps represents more abstract and high-level features than the layer before. Because of the increasing larger scale of these features \"deeper\" in the network, we can decrease the resolution of the feature maps and still retain quite of a bit of information about the original image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn42454obzQ_",
        "colab_type": "text"
      },
      "source": [
        "## Pooling operation\n",
        "\n",
        "The simplest method to downsample feature maps is the max-pool operation. Similar to a convolutional filter, a max-pool operation checks predefined subregions of an input matrix and simply outputs the maximum value (while discarding all other values). Recall that this operation must be used in combination with a stride value > 1 to actually downsample a given input matrix. An example of a 2 x 2 max-pool operation with a stride of 2 is shown below:\n",
        "\n",
        "![Max-Pool](https://raw.githubusercontent.com/CAIDMRes/images/master/assignment_05-03.png)\n",
        "\n",
        "To define a max-pool operation in Tensorflow, simply keep in mind the standard `N x H x W x C` convention for describing kernel sizes and strides. As before, `N` and `C` will almost always be 1 except for the most specialized use cases, with variations in kernel size primarily being defined along the `H x W` dimensions. Again, keep in mind that stride value > 1 is also required to perform downsampling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoHJTo0EvU21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define inputs\n",
        "im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "w1 = tf.placeholder(tf.float32, [5, 5, 1, 16])\n",
        "\n",
        "# Perform convolution + ReLU\n",
        "output = tf.nn.relu(tf.nn.conv2d(im, w1, strides=[1, 1, 1, 1], padding='SAME'))\n",
        "\n",
        "# Perform max-pool\n",
        "output = tf.nn.max_pool(output, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "# What is the size of the output matrix?\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn_EdIV4wEQE",
        "colab_type": "text"
      },
      "source": [
        "Let's see if you can solve through these additional examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Tt33hjwG3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ==============================================================================\n",
        "# Exercise 1 \n",
        "# ==============================================================================\n",
        "# \n",
        "# Define a max-pool operation with a kernel size of [1, 4, 4, 1] and a stride\n",
        "# of [1, 4, 4, 1]. What is the output matrix shape?\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "w1 = tf.placeholder(tf.float32, [5, 5, 1, 16])\n",
        "output = tf.nn.relu(tf.nn.conv2d(im, w1, strides=[1, 1, 1, 1], padding='SAME'))\n",
        "output = tf.nn.max_pool(?)\n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7AOcviWTYST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ==============================================================================\n",
        "# Exercise 2 \n",
        "# ==============================================================================\n",
        "# \n",
        "# Define a max-pool operation with a kernel size of [1, 4, 4, 1] and a stride\n",
        "# of [1, 2, 2, 1]. What is the output matrix shape?\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "w1 = tf.placeholder(tf.float32, [5, 5, 1, 16])\n",
        "output = tf.nn.relu(tf.nn.conv2d(im, w1, strides=[1, 1, 1, 1], padding='SAME'))\n",
        "output = tf.nn.max_pool(?)\n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gebObRUuwujB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ==============================================================================\n",
        "# **ADVANCED** \n",
        "# ==============================================================================\n",
        "# \n",
        "# Define an average pool operation with kernel size of [1, 2, 2, 1] and a stride\n",
        "# of [1, 2, 2, 1]. What is the name of the function in Tensorflow for this \n",
        "# operation? How does this operation differ from a standard max-pool operation?\n",
        "# If we change the kernel size to [1, 4, 4, 1] while keeping the stride unchanged,\n",
        "# what happens to the output matrix shape? What is the primary difference to the \n",
        "# first operation?\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "w1 = tf.placeholder(tf.float32, [5, 5, 1, 16])\n",
        "output = tf.nn.relu(tf.nn.conv2d(im, w1, strides=[1, 1, 1, 1], padding='SAME'))\n",
        "output = ? \n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oedca0HJb1Os",
        "colab_type": "text"
      },
      "source": [
        "## Strided convolutions\n",
        "\n",
        "A second method to downsample feature maps is the strided convolution. Just like the way a max-pool (or avg-pool) operation with stride 2 downsamples a feature by a factor of 2 in the height and width dimensions, a convolution (of any kernel size) with stride 2 can to be used to accomplish the same effect. See discussion above regarding convolutional math, specifically example #1, for further information.\n",
        "\n",
        "## Putting it all together\n",
        "\n",
        "Let's put together a series of convolutions, non-linearities and downsampling operations together. Starting with a `[None, 28, 28, 1]` input matrix, let us perform the following:\n",
        "\n",
        "**Block 1**\n",
        "* convolution via a total of 16 filters each of size 5 x 5\n",
        "* ReLU non-linearity\n",
        "* max-pool operation with a `[1, 2, 2, 1]` kernel and `[1, 2, 2, 1]` stride\n",
        "\n",
        "**Block 2**\n",
        "* convolution via a total of 32 filters each of size 5 x 5\n",
        "* ReLU non-linearity\n",
        "* max-pool operation with a `[1, 2, 2, 1]` kernel and `[1, 2, 2, 1]` stride\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aglR8Bl9ZSq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_blocks():\n",
        "    \n",
        "    # Define inputs\n",
        "    im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "    w1 = tf.placeholder(tf.float32, ?)\n",
        "    w2 = tf.placeholder(tf.float32, ?)\n",
        "\n",
        "    # Block 1\n",
        "    output = tf.nn.relu(tf.nn.conv2d(?))\n",
        "    output = tf.nn.max_pool(?)        \n",
        "\n",
        "    # Block 2 \n",
        "    output = tf.nn.relu(tf.nn.conv2d(?)\n",
        "    output = tf.nn.max_pool(?)        \n",
        "    \n",
        "    return im, output, [w1, w2]\n",
        "                    \n",
        "im, output, weights = create_blocks()\n",
        "\n",
        "# What is the size of the output matrix?\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNu8pn2Nb55l",
        "colab_type": "text"
      },
      "source": [
        "# Fully Connected Layers\n",
        "\n",
        "After a series of convolutions, non-linearities and downsampling operations, the original 28 x 28 MNIST image will be collapsed to a number number of small feature maps (in the above example a 7 x 7 x 32 matrix). However recall that the goal remains to collapse the image even further, specifically, to a total of 10 logit scores for digit prediction. How do convert a 7 x 7 x 32 size feature map to 10 different logit scores?\n",
        "\n",
        "While there are a handful of strategies to do this, a popular approach is to convert the 2D feature maps into a single vector and simply use matrix multiplications until we reach our final logit scores (just like regular neural networks). In other words, we can reshape for example a 7 x 7 x 32 matrix into a 1 x 1568-element vector and multiply it by a 1568 x 10 size matrix to arrive at our final 10-element logit score. As a convention, every layer after which a CNN \"converts\" into a regular neural network is known as a **fully connected** layer because at this point, every node in a hidden layer becomes connected to every node that came before it. This is contrast to convolutional layers, where each element in a feature map arise only from the neurons in a small receptive field in a small portion of the image or feature map that came before it. \n",
        "\n",
        "Let's see an example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMrWQ_M9cmOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use create_blocks()\n",
        "im, output, weights = create_blocks()\n",
        "\n",
        "# Reshape (\"flatten\") matrix\n",
        "flattened = tf.reshape(output, shape=[-1, 7 * 7 * 32])\n",
        "\n",
        "# Matrix multiply\n",
        "w3 = tf.placeholder(tf.float32, [7 * 7 * 32, 10])\n",
        "logits = tf.matmul(flattened, w3)\n",
        "\n",
        "# What is the size of the output matrix?\n",
        "print(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYXa5GpKeFZk",
        "colab_type": "text"
      },
      "source": [
        "How would you add one \"hidden\" layer to this model? Don't forget to add a ReLU non-linearity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPTjTHzNfRT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use create_blocks()\n",
        "im, output, weights = create_blocks()\n",
        "\n",
        "# Reshape (\"flatten\") matrix\n",
        "flattened = tf.reshape(?)\n",
        "\n",
        "# Matrix multiply #1, hidden layer size 128 (don't forget ReLU)\n",
        "w3 = tf.placeholder(tf.float32, ?)\n",
        "hidden = ? \n",
        "\n",
        "# Matrix multiply #2, output logits size 10 (no ReLU for logits)\n",
        "w4 = tf.placeholder(tf.float32, ?)\n",
        "logits = ? \n",
        "\n",
        "# What is the size of the output matrix?\n",
        "print(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UF-OFFVb8ly",
        "colab_type": "text"
      },
      "source": [
        "# Training a Convolutional Neural Network\n",
        "\n",
        "Congratulations! At this point you're ready to train your neural network. We will use the same base architecture as above: two serial Conv-ReLU-MaxPool blocks followed by a matrix reshape, single 128-node hidden layer and 10-element logit score. Once we collapse our image into a 10-element logit score, we can use our basic softmax cross-entropy loss function and Adam optimizer to train the network. Let's do it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egxDBjKqcIKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    \"\"\"\n",
        "    Method to create the following CNN architecture:\n",
        "    \n",
        "      - BLOCK 1 (filter depth of 16)\n",
        "      - BLOCK 2 (filter depth of 32)\n",
        "      - RESHAPE\n",
        "      - HIDDEN LAYER (128 nodes)\n",
        "      - LOGIT SCORES (10 nodes)\n",
        "      \n",
        "    Note that a BLOCK consists of a Conv-ReLU-MaxPool combination where\n",
        "    convolutional kernels are all 5 x 5 in shape.\n",
        "    \n",
        "    \"\"\"\n",
        "    # Reset our graph to build a new one\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define placeholders for our images and labels\n",
        "    # ------------------------------------------------------------------------\n",
        "    #\n",
        "    # 1. Define images\n",
        "    # \n",
        "    #  - type: float32\n",
        "    #  - size: [None, 28, 28, 1] so that we can feed in as many images as we need\n",
        "    # \n",
        "    # 2. Define labels\n",
        "    # \n",
        "    #  - type: int64\n",
        "    #  - size: [None] so that we can feed in as many labels as we need\n",
        "    # \n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "    labels = tf.placeholder(tf.int64, [None])\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define convolutional blocks \n",
        "    # ------------------------------------------------------------------------\n",
        "    # \n",
        "    # A convolutional block is defined by a series of convolution, ReLU and \n",
        "    # max-pool operations performed consecutively. Each block is defined by a set \n",
        "    # number of filters (feature map depth) that is set by the convolutional\n",
        "    # kernel size. Recall above that the convolutional kernel size must reflect\n",
        "    # both the number of feature maps in the layer before as well as the number of \n",
        "    # feature maps in the layer afterwards.\n",
        "    #\n",
        "    # As in previous assignments, we will use the tf.get_variables(...) method\n",
        "    # to create matrix variables initialized to random values.\n",
        "    # \n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Block 1 (use a filter depth of 16)\n",
        "    w1 = tf.get_variable('w1', shape=?, dtype=tf.float32)\n",
        "    layer = tf.nn.relu(tf.nn.conv2d(im, w1, strides=?, padding='SAME'))\n",
        "    layer = tf.nn.max_pool(layer, ksize=?, strides=?, padding='SAME')        \n",
        "\n",
        "    # Block 2 (use a filter depth of 32)\n",
        "    w2 = tf.get_variable('w2', shape=?, dtype=tf.float32)\n",
        "    layer = tf.nn.relu(tf.nn.conv2d(layer, w2, strides=?, padding='SAME'))\n",
        "    layer = tf.nn.max_pool(layer, ksize=?, strides=?, padding='SAME')        \n",
        "    \n",
        "    # ------------------------------------------------------------------------\n",
        "    # Reshape to 1D vector \n",
        "    # ------------------------------------------------------------------------\n",
        "    \n",
        "    flattened = tf.reshape(layer, [?])\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our matmul operations\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Hidden layer (128 nodes)\n",
        "    w3 = tf.get_variable('w3', shape=?, dtype=tf.float32)\n",
        "    h1 = tf.nn.relu(tf.matmul(flattened, w3))\n",
        "    \n",
        "    # Logits (10 nodes)\n",
        "    w4 = tf.get_variable('w4', shape=?, dtype=tf.float32)\n",
        "    logits = tf.matmul(h1, w4)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our softmax cross-entropy loss\n",
        "    # ------------------------------------------------------------------------\n",
        "    # \n",
        "    # HINT: use tf.losses.sparse_softmax_cross_entropy() as above\n",
        "    #\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our optimizer\n",
        "    # ------------------------------------------------------------------------\n",
        "    # \n",
        "    # An optimizer is a special TensorFlow class that takes your model weights and \n",
        "    # adjusts them ever so slightly so that they will make a better prediction the\n",
        "    # next time around. They are implemented with a technique known as \n",
        "    # backpropogation which we will learn about in further detail during later \n",
        "    # lectures. For now, just know that this is what we are using here.\n",
        "    #\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
        "    \n",
        "    return im, labels, [w1, w2, w3, w4], logits, loss, train_op\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Test our model\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# If the graph was defined properly, we should be able to check the out\n",
        "# what the model outputs should look like. Can you guess by the shapes\n",
        "# of our logits and losses will be?\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "im, labels, weights, logits, loss, train_op = create_model()\n",
        "print(logits.shape)\n",
        "print(loss.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yeQFY_Rjn5u",
        "colab_type": "text"
      },
      "source": [
        "## Intialization\n",
        "\n",
        "Now we set up some code to initialize our network graph, variables and new saver object. This code is identical to the earlier assignments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoKTxo5LcJYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Create our model\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "im, labels, weights, logits, loss, train_op = create_model()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Add to collections\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# Collections are used by TensorFlow to keep track of certain intermediate \n",
        "# values for quick access during save/load functions.\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "tf.add_to_collection('im', im)\n",
        "tf.add_to_collection('logits', logits)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Initialize our test graph\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# What two things do we need to initialize our graph?\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Initialize our test graph\n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# Initialize a Saver object\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFF7d5-kjyzm",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Let's train our algorithm! The code here is nearly identical to our earlier method except that in this case we must reshape our input 784-element vector to a 28 x 28 x 1 image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuXgVOYzcPag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Train our algorithm \n",
        "# ------------------------------------------------------------------------\n",
        "# \n",
        "# Let's set up a loop to train our algorithm by feeding it data iteratively.\n",
        "# For each iteration, we will feed a batch_size number of images into our \n",
        "# model and let it readjust it's neuronal weights.\n",
        "# \n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "def train_model(iterations=1000, batch_size=256):\n",
        "    \n",
        "    accuracies = []\n",
        "    losses = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Grab a total of batch_size number of random images and labels \n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # 1. Pick batch_size number of random indices between 0 and 60,000\n",
        "        # 2. Select those images / labels\n",
        "        #\n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        rand_indices = np.random.randint(60000, size=(batch_size))\n",
        "        x_batch = x[?].reshape(batch_size, 28, 28, 1)\n",
        "        y_batch = y[?]\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Normalize x_batch\n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # Currently, values in x range from 0 to 255. If we normalize these values\n",
        "        # to a mean of 0 and SD of 1 we will improve the stability of training\n",
        "        # and furthermore improve interpretation of learned weights. Use the\n",
        "        # following code to normalize your batch:\n",
        "        # \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        x_batch = (x_batch - np.mean(x_batch)) / np.std(x_batch)\n",
        "\n",
        "        # Convert to types matching our defined placeholders\n",
        "        x_batch = x_batch.astype('float32')\n",
        "        y_batch = y_batch.astype('int64')\n",
        "\n",
        "        # Prepare feed_dict\n",
        "        feed_dict = {im: x_batch, labels: y_batch}\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Run training iteration via sess.run()\n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # Here, in addition to whichever ouputs we wish to extract, we need to\n",
        "        # also include the train_op variable. Including train_op will tell \n",
        "        # Tensorflow that in addition to calculating the intermediates of our graph,\n",
        "        # we also need to readjust the variables so that the overall loss goes\n",
        "        # down.\n",
        "        # \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        outputs = sess.run([logits, loss, train_op], ?)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Use argmax to determine highest logit (model guess)\n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # Keep in mind our logits matrix is (batch_size x 10) in size representing\n",
        "        # a total of batch_size number of predictions. How do we process this matrix\n",
        "        # with the np.argmax() to find the highest logit along each row of the matrix\n",
        "        # (e.g. find the prediction for each of our images)?\n",
        "        # \n",
        "        # HINT: what does the axis parameter in np.argmax(a, axis) specify?\n",
        "        # \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        predictions = np.argmax(outputs[0], axis=1)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Calculate accuracy \n",
        "        # --------------------------------------------------------------------\n",
        "        # \n",
        "        # Consider the following:\n",
        "        # \n",
        "        # - predictions = the predicted digits\n",
        "        # - y_batch = the ground-truth digits\n",
        "        # \n",
        "        # How do I calculate an accuracy % with this data?\n",
        "        # \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        accuracy = np.sum(predictions == y_batch) / batch_size\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Accumulate and print iteration, loss and accuracy \n",
        "        # --------------------------------------------------------------------\n",
        "\n",
        "        print('Iteration %05i | Loss = %07.3f | Accuracy = %0.4f' %\n",
        "            (i + 1, outputs[1], accuracy))\n",
        "\n",
        "        losses.append(outputs[1])\n",
        "        accuracies.append(accuracy)\n",
        "        \n",
        "    return losses, accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdgptfdWcVB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------------------------------------------------------\n",
        "# Train model\n",
        "# --------------------------------------------------------------------\n",
        "losses, accuracies = train_model(iterations=1000, batch_size=256)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Graph outputs and accuracy\n",
        "# --------------------------------------------------------------------\n",
        "import pylab\n",
        "pylab.plot(losses)\n",
        "pylab.title('Model loss over time')\n",
        "pylab.show()\n",
        "\n",
        "pylab.plot(accuracies)\n",
        "pylab.title('Model accuracy over time')\n",
        "pylab.show()\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Save model\n",
        "# --------------------------------------------------------------------\n",
        "# \n",
        "# In this step, all model variables and the underlying graph structure\n",
        "# are saved so that they can be reloaded. Although it looks like just one\n",
        "# file is saved here, in fact both a *.cpkt and *.cpkt.meta file are both\n",
        "# saved in this single line of code.\n",
        "#  \n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "model_file = './model_cnn_16_32_128/model.ckpt'\n",
        "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
        "print('Saving model')\n",
        "saver.save(sess, model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovOl0JE1cdxG",
        "colab_type": "text"
      },
      "source": [
        "# Running inference\n",
        "\n",
        "Now that we have a trained model, let's go ahead and see how it performs! We will use the same procedure as before to load up a trained network and then feed in random digits to see how it fares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnUhBDI2chqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the saved model\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "saver = tf.train.import_meta_graph('./model_cnn_16_32_128/model.ckpt.meta')\n",
        "saver.restore(sess, './model_cnn_16_32_128/model.ckpt')\n",
        "\n",
        "# Find our placeholders\n",
        "im = tf.get_collection('im')[0]\n",
        "logits = tf.get_collection('logits')[0]\n",
        "\n",
        "# Find a random test image\n",
        "i = int(np.random.randint(60000))\n",
        "image = x[i].reshape(1, 28, 28, 1)\n",
        "label = y[i]\n",
        "\n",
        "# Normalize the image\n",
        "image = (image - np.mean(image)) / np.std(image)\n",
        "\n",
        "# Create a feed_dict\n",
        "feed_dict = {im: image}\n",
        "\n",
        "# Pass data through the network\n",
        "l = sess.run(logits, feed_dict)\n",
        "\n",
        "# Convert logits to predictions\n",
        "prediction = np.argmax(l)\n",
        "\n",
        "# Visualize\n",
        "pylab.imshow(image.reshape(28, 28))\n",
        "pylab.axis('off')\n",
        "pylab.title('My prediction is %i' % prediction)\n",
        "pylab.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oypH0QDpcnGY",
        "colab_type": "text"
      },
      "source": [
        "# Model validation\n",
        "\n",
        "As we learned in lecture #4, a model with *too much* learning capacity can potentially memorize the dataset without learning anything too useful. This was certainly a small but definite problem with our regular non-convolutional neural networks. How do we fare with CNNs? To test for this phenomenon we need to evaluate the model on new data that the algorithm has never seen before. Let's go ahead download this new data now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_gB9cDVco36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/CAIDMRes/lecture_03\n",
        "!unzip lecture_03/data.zip\n",
        "!rm -r lecture_03 \n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsBVUw_act7H",
        "colab_type": "text"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "The two new files we downloaded are `x_test.npy` and `y_test.npy` corresponding to our test set data and labels.  The format is identical to before. We have a total of 10,000 examples to test. Let us load them now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-NQsafcc1a1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "x_test = np.load('x_test.npy')\n",
        "y_test = np.load('y_test.npy')\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZreX2Sec4fC",
        "colab_type": "text"
      },
      "source": [
        "## Validating\n",
        "\n",
        "Using the template code to run inference shown above, let us now write code to:\n",
        "\n",
        "* load our saved model \n",
        "* create a `feed_dict` with new test data \n",
        "* pass through network using `sess.run()`\n",
        "* convert the `logits` to predictions\n",
        "* calculate overall network accuracy\n",
        "\n",
        "This is nearly identical to our previous assignments, with the exception that our input images are now 2D (28 x 28 x 1) instead of 1D vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrdaSgVtc8Sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_model(model_file):\n",
        "    \"\"\"\n",
        "    Method to test the validation performance of a model using the \n",
        "    test set data.\n",
        "    \n",
        "    :params\n",
        "    \n",
        "      (str) model_file : name of model file saved by saver object\n",
        "      \n",
        "    \"\"\"\n",
        "    # Load saved model\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.InteractiveSession()\n",
        "    saver = tf.train.import_meta_graph('%s.meta' % model_file )\n",
        "    saver.restore(sess, model_file)\n",
        "\n",
        "    # Find our placeholders\n",
        "    im = tf.get_collection('im')[0]\n",
        "    logits = tf.get_collection('logits')[0]\n",
        "\n",
        "    # Normalize our input data x_test\n",
        "    input_data = (x_test - np.mean(x_test, axis=1, keepdims=True)) / \\\n",
        "        np.std(x_test, axis=1, keepdims=True)\n",
        "    \n",
        "    # -------------------------------------------------------\n",
        "    # Create a feed_dict\n",
        "    # -------------------------------------------------------\n",
        "    # \n",
        "    # HINT: What do we need to do to properly format this image\n",
        "    # for input into the CNN?\n",
        "    # \n",
        "    # -------------------------------------------------------\n",
        "\n",
        "    feed_dict = {im: ?}\n",
        "\n",
        "    # Pass data through the network using sess.run() to get our logits \n",
        "    output = sess.run(logits, feed_dict)\n",
        "\n",
        "    # Convert logits to predictions\n",
        "    predictions = np.argmax(output, axis=1)\n",
        "\n",
        "    # Compare predictions to ground-truth to find accuracy\n",
        "    accuracy = np.sum(predictions == y_test) / x_test.shape[0]\n",
        "    \n",
        "    print('Network test-set accuracy: %0.4f' % accuracy)\n",
        "    \n",
        "# Pass our model_file\n",
        "model_file = './model_cnn_16_32_128/model.ckpt' \n",
        "validate_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_70CkObXdE39",
        "colab_type": "text"
      },
      "source": [
        "## Notes\n",
        "\n",
        "How did the algorithm perform? Better or worse than our non-convolutional neural network? Were you surprised, not surprised? In the remainder of this tutorial, let's test a handful of different architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUyXYZlEdJId",
        "colab_type": "text"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "For the following exercises, we will evaluate a number of different variations in CNN architecture. As before, the steps will include:\n",
        "\n",
        "* writing a new model in the `create_model()` method\n",
        "* initial training variables\n",
        "* use the `train_model()` method defined above to run training (repeated as many times needed to converge)\n",
        "* save model\n",
        "* validate model on test set data\n",
        "\n",
        "The goal is to get a sense of which combinations work better than others. Keep in mind we are already at 99%+ accuracy, so we're not expecting any dramatic changes, but the process fine-tuning a neural network is an extremely valuable experience to gain first-hand.\n",
        "\n",
        "## Exercise 1\n",
        "\n",
        "Re-train several neural networks this time with either more or less convolutional filters (e.g. 32-64 or 8-16). What do you expect to happen to your algorithm accuracy? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BXQV4NTq0hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    \n",
        "    # Reset our graph to build a new one\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define placeholders for our images and labels\n",
        "    # ------------------------------------------------------------------------\n",
        "    im = ? \n",
        "    labels = ?\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define convolutional blocks \n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Block 1\n",
        "    w1 = ?\n",
        "    layer = tf.nn.relu(tf.nn.conv2d(?))\n",
        "    layer = tf.nn.max_pool(?)        \n",
        "\n",
        "    # Block 2\n",
        "    w2 = ?\n",
        "    layer = tf.nn.relu(tf.nn.conv2d(?))\n",
        "    layer = tf.nn.max_pool(?)        \n",
        "    \n",
        "    # ------------------------------------------------------------------------\n",
        "    # Reshape to 1D vector \n",
        "    # ------------------------------------------------------------------------\n",
        "    flattened = ?\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our matmul operations\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Hidden layer (128 nodes)\n",
        "    w3 = ?\n",
        "    h1 = ?\n",
        "    \n",
        "    # Logits (10 nodes)\n",
        "    w4 = ?\n",
        "    logits = ?\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our softmax cross-entropy loss\n",
        "    # ------------------------------------------------------------------------\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our optimizer\n",
        "    # ------------------------------------------------------------------------\n",
        "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
        "    \n",
        "    return im, labels, [w1, w2, w3, w4], logits, loss, train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEHMKch2rJnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Create model \n",
        "# ------------------------------------------------------------------------\n",
        "im, labels, weights, logits, loss, train_op = create_model()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Add to collections, initialize graph and saver\n",
        "# ------------------------------------------------------------------------\n",
        "tf.add_to_collection('im', im)\n",
        "tf.add_to_collection('logits', logits)\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG3Y5Cbkse-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------------------------------------------------------\n",
        "# Train model\n",
        "# --------------------------------------------------------------------\n",
        "losses, accuracies = train_model(iterations=1000, batch_size=256)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Graph outputs and accuracy\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "pylab.plot(losses)\n",
        "pylab.title('Model loss over time')\n",
        "pylab.show()\n",
        "\n",
        "pylab.plot(accuracies)\n",
        "pylab.title('Model accuracy over time')\n",
        "pylab.show()\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Save model\n",
        "# --------------------------------------------------------------------\n",
        "# \n",
        "# In this step, all model variables and the underlying graph structure\n",
        "# are saved so that they can be reloaded. Although it looks like just one\n",
        "# file is saved here, in fact both a *.cpkt and *.cpkt.meta file are both\n",
        "# saved in this single line of code.\n",
        "#  \n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "model_file = ?\n",
        "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
        "print('Saving model')\n",
        "saver.save(sess, model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3baMHBPrKBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Validate model \n",
        "# ------------------------------------------------------------------------\n",
        "validate_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DHvUjAxmqzT",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Re-train several neural networks this time with either increased or decreased size of hidden layers (perhaps 196, 256, or 32, 64). Alternatively, try increasing the number of hidden layers, or completely remove the hidden layer. What do you expect to happen to your algorithm accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK63bzQ-tJ_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    \n",
        "    # Reset our graph to build a new one\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define placeholders for our images and labels\n",
        "    # ------------------------------------------------------------------------\n",
        "    im = ? \n",
        "    labels = ?\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define convolutional blocks \n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Block 1\n",
        "    w1 = ?\n",
        "    layer = tf.nn.relu(tf.nn.conv2d(?))\n",
        "    layer = tf.nn.max_pool(?)        \n",
        "\n",
        "    # Block 2\n",
        "    w2 = ?\n",
        "    layer = tf.nn.relu(tf.nn.conv2d(?))\n",
        "    layer = tf.nn.max_pool(?)        \n",
        "    \n",
        "    # ------------------------------------------------------------------------\n",
        "    # Reshape to 1D vector \n",
        "    # ------------------------------------------------------------------------\n",
        "    flattened = ?\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our matmul operations\n",
        "    # ------------------------------------------------------------------------\n",
        "\n",
        "    # Hidden layer\n",
        "    w3 = ?\n",
        "    h1 = ?\n",
        "    \n",
        "    # Logits (10 nodes)\n",
        "    w4 = ?\n",
        "    logits = ?\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our softmax cross-entropy loss\n",
        "    # ------------------------------------------------------------------------\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "    # ------------------------------------------------------------------------\n",
        "    # Define our optimizer\n",
        "    # ------------------------------------------------------------------------\n",
        "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
        "    \n",
        "    return im, labels, [w1, w2, w3, w4], logits, loss, train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOTGhzaUtQhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Create model \n",
        "# ------------------------------------------------------------------------\n",
        "im, labels, weights, logits, loss, train_op = create_model()\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Add to collections, initialize graph and saver\n",
        "# ------------------------------------------------------------------------\n",
        "tf.add_to_collection('im', im)\n",
        "tf.add_to_collection('logits', logits)\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypBxACnDtT9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --------------------------------------------------------------------\n",
        "# Train model\n",
        "# --------------------------------------------------------------------\n",
        "losses, accuracies = train_model(iterations=1000, batch_size=256)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Graph outputs and accuracy\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "pylab.plot(losses)\n",
        "pylab.title('Model loss over time')\n",
        "pylab.show()\n",
        "\n",
        "pylab.plot(accuracies)\n",
        "pylab.title('Model accuracy over time')\n",
        "pylab.show()\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Save model\n",
        "# --------------------------------------------------------------------\n",
        "# \n",
        "# In this step, all model variables and the underlying graph structure\n",
        "# are saved so that they can be reloaded. Although it looks like just one\n",
        "# file is saved here, in fact both a *.cpkt and *.cpkt.meta file are both\n",
        "# saved in this single line of code.\n",
        "#  \n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "model_file = ?\n",
        "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
        "print('Saving model')\n",
        "saver.save(sess, model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL2Tek4UtW8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Validate model \n",
        "# ------------------------------------------------------------------------\n",
        "validate_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vxvdw7hnHaE",
        "colab_type": "text"
      },
      "source": [
        "# Advanced Exercises\n",
        "\n",
        "Up for a challenge? Go ahead and try some (or all) of these out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FGx9e1KnP_f",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "In this example, we defined a convolutional block as a single series of convolution, ReLU and down-sampling. What happens if we use two convolutional operations, in other words convolution, ReLU, convolution, ReLU, down-sampling?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXS23WlQqqH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxdquaijn3R9",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "In this example, the method to collapse our intermediate 7 x 7 x 28 feature map was the reshape and treat the network as a regular ANN. Can you think of alternatives to this approach? Remember the only rule is that we must end up with a 10-element logit score.\n",
        "\n",
        "Some thoughts:\n",
        "\n",
        "* additional application of a single convolution block, average pool and/or strided convolution to downample the feature map to 3 x 3 x N before proceeding with the matrix reshape\n",
        "* continued application of convolution blocks, average pools and/or strided convolutions  to downsample the feature map to 1 x 1 x N, and subsequently treating the end result as a single dimensional hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP-I3hD7qpqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLVdEtYRpt2w",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 3\n",
        "\n",
        "Why does the CNN perform better than the ANN? Specifically, why does the algorithm overfit less? Calculate the number of total **trainable parameters** (total elements in all weight combined) for a CNN vs. ANN approach. Any other thoughts?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_LF-nPZqn81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4LwMo58qNbG",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 4\n",
        "\n",
        "As many of you recognized in the first assignment, an *ensemble* of algorithms (e.g. multiple algorithms trained with slightly different architectures) tends to perform better than any single one model. Create an ensemble of your top CNNs (and/or ANNs), and see you if you can push your model accuracy even further"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRHcLDv6qpGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}