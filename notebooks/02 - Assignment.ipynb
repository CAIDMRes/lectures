{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02 - Assignment.ipynb","version":"0.3.2","provenance":[{"file_id":"1daTob7CLEmhJoTUvKdy0CYCZM4QpyzCu","timestamp":1531612886453}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kkLSHU9l8Pld","colab_type":"text"},"source":["# Overview\n","\n","In this assignment we will take two very large steps forward in understanding how to implement a neural network algorithm. In the first part, we will review some *very basic* concepts related to math and matrix algrebra, just enough so that we can translate what we learned in lectures 1 and 2 into a mathematic formulation. In the second part we will introduce the TensorFlow library, maintained by the Google Brain team, to actually design and train our basic classifier.\n","\n","### Math and Matrix Algebra Review\n","\n","* matrix multiplication\n","* softmax scores\n","* cross-entropy\n","\n","### TensorFlow\n","\n","* overview of graphs\n","* training a model\n","* inference from a model\n","\n","## Restarting your Virtual Machine\n","\n","If at any point during this assignment you accidentally execute code or do something that cannot seem to undo and need to \"restart\" the system (including deleting all temporary folders), go ahead and run the following single line of code. It will take about 1 minute to restart. Following this, you will have to proceed at the beginning of the assignment to re-downloaded the data and run the code you have written. Note, the code that you have already written will **not** be deleted; you simply need to start executing the code once again from the start."]},{"cell_type":"code","metadata":{"id":"HD-MR0SyJHUX","colab_type":"code","colab":{}},"source":["!kill -9 -1 # Warning this restarts your machine"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"63kEEoJFJHuf","colab_type":"text"},"source":["## Downloading the Data\n","\n","The following commands can be used to copy over the assignment materials to your local Colaboratory instance and unzip in preparation for your assignment:"]},{"cell_type":"code","metadata":{"id":"C-mmnvkcJIlv","colab_type":"code","colab":{}},"source":["!git clone https://github.com/CAIDMRes/lecture_02\n","!unzip lecture_02/data.zip\n","!rm -r lecture_02\n","!ls"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wcpSsUR4JGhT","colab_type":"text"},"source":["\n","\n","# Math and Matrix Algebra\n","\n","## Foundations\n","\n","To review the structure of a neural network, let us keep in mind the diagramatic representation of a neural network covered  in lecture 2:\n","\n","![Diagramtric Representation](https://raw.githubusercontent.com/CAIDMRes/images/master/assignment_02-01.png)\n","\n","Recall that each reach \"retinal neuron\" recieving light from the image is connected to one of ten outputs representing the likelihood that the image represents one of ten possible digits. The neuron with the largest number, also known as **logit score**, gets to decide what the final image is most likely to represent.\n","\n","Also, recall that each connection itself is modeled by a weight value that represents how strong (or weak) the connection is, and that each of the 784 connections to one of our ten output neurons can in fact be represented as a matrix of numbers:\n","\n","![Matrix Representation](https://raw.githubusercontent.com/CAIDMRes/images/master/assignment_02-02.png)\n","\n","Of course, for each of our ten digits, we will have a different 28 x 28 weight matrix for a total of 10 weight matrices. Finally keep in mind that for the sake of ease in representation (given our simple model) both our 28 x 28 input matrix and ten 28 x 28 weight matrices can be **flattened** to just a single 784 x 1 (or 1 x 784) matrix. \n","\n","## NumPy Matrix Algebra\n","\n","A 2D matrix is composed of **rows** and **columns**. The standard notation for referencing the size of a matrix is (# of rows) x (# of columns.) Let's start with a hypothetical toy 3 x 3 image:"]},{"cell_type":"code","metadata":{"id":"JHi9sxDwCjyY","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","im = np.array([\n","    [1, 2, 3],\n","    [4, 5, 6],\n","    [7, 8, 9]\n","])\n","\n","print(im)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q_9bG7LP_ccU","colab_type":"text"},"source":["Recall now the basic rules for matrix multiplication:\n","\n","![Matrix Multiplication](https://www.mathsisfun.com/algebra/images/matrix-multiply-b.svg)\n","\n","1. Matrix **A** can be multipled by matrix **B** if the number of columns in A = number of rows in B. In the above example a 2 x 3 matrix can be multipled by a 3 x 2 because the second number (columns) in **A** is equal to the first number (rows) in **B**.\n","2. Matrix **A** multipled by matrix **B** results in an output matrix **C** with a size that is equal to the (number of rows in **A**) x (number of columns in **B**). In the above example, the result of a 2 x 3 matrix multipled by a 3 x 2 matrix is 2 x 2.\n","3. To arrive at the (i, j)-th entry in matrix **C**, multiply and add together the i-*th* row in **A** by the j-*th* column in **B**. In the above example the entry in the first row / second column of **C** (also referenced as C[1, 2]) can be obtained by multiplying then adding the first row in **A** by the second column in **B**:\n","```\n","C[1,2] = 1*8 + 2*10 + 3*12 = 8 + 20 + 36 = 64\n","```\n","\n","Given this, what is the output shape and result of im x im?\n","\n"]},{"cell_type":"code","metadata":{"id":"WEYO1pCSIjay","colab_type":"code","colab":{}},"source":["# Use np.matmul() for matrix multiply\n","c = np.matmul(im, im)\n","\n","print(c.shape)\n","\n","print(c)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQRtZUu0LLlW","colab_type":"text"},"source":["## Using MNIST data\n","\n","Let's go ahead and load up the MNIST dataset from lecture 2. Recall that `x` is a matrix containing all the images and `y` is a matrix containing all the labels."]},{"cell_type":"code","metadata":{"id":"vN0N4Id1Nt8v","colab_type":"code","colab":{}},"source":["# Loading a pickle (*.pkl) file\n","import pickle\n","x = pickle.load(open('x.pkl', 'rb'))\n","\n","# x is a NumPy array with (flattened) image data\n","print(type(x))\n","print(x.shape)\n","\n","# y is a NumPy array with labels\n","y = pickle.load(open('y.pkl', 'rb'))\n","print(y.shape) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6XPGP9mxONTV","colab_type":"text"},"source":["As we see here `x` is a (60,000 x 784) matrix with all our imaging data. As we learned in the previous section, this matrix contains 60,000 rows and 784 columns; each row represents one image (e.g. `x[0], x[1], x[2], ...` etc). Let's take a look at the first image / label in the dataset:"]},{"cell_type":"code","metadata":{"id":"uq9XVP_-OS7U","colab_type":"code","colab":{}},"source":["# This is our first image\n","im = x[0]\n","im = im.reshape(28, 28)\n","\n","# This is our first label\n","label = y[0]\n","\n","# Let's visualize\n","import pylab\n","pylab.imshow(im)\n","pylab.axis('off')\n","pylab.title('This is an example of the digit ' + str(label))\n","pylab.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APbbFFleQI0Y","colab_type":"text"},"source":["## Matrix multiply\n","\n","As described in above, each one of these input \"retinal neurons\" (one for each pixel in the image) gets connected to one of ten output neurons via a weight. Let's consider for example the single output neuron in charge of predicting that the number is zero. This neuron synthesizes the input of a total of 784 weights, one for each pixel, and combines them all into one final **logit score**. This can be implemented via a simple matrix multiply. Let's define the following:"]},{"cell_type":"code","metadata":{"id":"dLhnLk4RR30e","colab_type":"code","colab":{}},"source":["# This is our first image\n","im = x[0]\n","\n","# This is 784 random weights from a distribution with mean = 0, SD = 0.0001 \n","weights = np.random.normal(loc=0, scale=0.0001, size=(784))\n","\n","# Let's look at the first 10 values of each\n","print(im[:10])\n","print(weights[:10])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ba8cG0eASl5h","colab_type":"text"},"source":["As expected, the first few values of an image matrix generally contain zeros (e.g. edge of the image), while the first few weights that we initialized randomly contain numbers drawn from a normal Gaussian distribution (bell curve) with a mean of 0 and standard deviation of 0.0001.\n","\n","How do we multiply these two matrices together and sum up the values so that we end up with just *one* final logit score? Recall that both matrices are (784 x 1) in size, so that breaks the rule for having matrix sizes properly match for multiplication (recall **columns** of A must = **rows** of B)? Do we need to do a (1 x 784) x (784 x 1) operation or (784 x 1) x (1 x 784) operation? \n","\n","The answer is that you need to a  (1 x 784) x (784 x 1). This as you remember, based on the rules of matrix multiplication, will result in each pixel value being multiplied by it's corresponding weight and summing the results up into a single cumulative value, which is exactly what we want. To code this in NumPy:"]},{"cell_type":"code","metadata":{"id":"twoXTNfQTMEl","colab_type":"code","colab":{}},"source":["A = im.reshape(1, 784)\n","B = weights.reshape(784, 1)\n","logit = np.matmul(A, B)\n","print(logit)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hEdP7QQ9VuXL","colab_type":"text"},"source":["What if we want to define ten total weight matrices? Instead of doing this ten different times, you can simply stack ten **rows** of matrices on top of each other, yielding a combined weight matrix of size 784 x 10. Then when we multiply our original (1 x 784) image by this combined weight matrix, we get *ten* different values representing the likelihood of each corresponding digit. If you need, refer back to the diagram for matrix multiplication above to prove to yourself this is what is happenening. To code this in NumPy:"]},{"cell_type":"code","metadata":{"id":"_1Hj3dF2WNQP","colab_type":"code","colab":{}},"source":["# This is 784 random weights from a distribution with mean = 0, SD = 0.0001 \n","weights = np.random.normal(loc=0, scale=0.0001, size=(784, 10))\n","\n","# One matrix multiply to get the logit score for all weights at one time\n","logits = np.matmul(A, weights)\n","\n","# Predictions is (1 x 10) in size; lets see what's inside\n","for i in range(10):\n","    print('The logit score for being a digit %i is: %.03f' % (i, logits[0, i]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ANfKi8xVX_L6","colab_type":"text"},"source":["## Softmax\n","\n","To translate raw score values into probabilities (likelihood between 0 and 1) we use a special formula known as the softmax. The softmax score is calculated as follows:\n","\n","1. For each logits score ***x***, convert that value to ***e ^ x*** (where ***e*** is the natural log base). This will represent that score **numerator**.\n","2. Add up all the converted scores. This will represent the score **denominator**.\n","3. Calculate each score by taking the numerator / denominator. If done correctly, all scores should add up to 1 (e.g. a valid probability distribution).\n","\n","Let's write the a softmax method together:"]},{"cell_type":"code","metadata":{"id":"6PqN7o-FZZdB","colab_type":"code","colab":{}},"source":["def softmax(logits):\n","    \"\"\"\n","    Method to convert raw logit scores into softmax probabilities\n","    \n","    :params\n","    \n","      (np.array) logits: a 1 x N array representing N total predictions\n","    \n","    \"\"\"\n","    # (1) Convert the predictions array to exponential version\n","    # HINT: use np.exp()\n","    e = ?\n","    \n","    # (2) Find the total sum of all exponentials\n","    s = ?\n","    \n","    # (3) Take the exponential version and divide it by the total sum\n","    prob = ?\n","    \n","    # (4) Return your result\n","    return prob\n","    \n","s = softmax(logits)\n","for i in range(10):\n","    print('The prob for being a digit %i is: %.03f' % (i, s[0, i]))\n","    \n","print('The sum of all probabilities is: %.03f' % np.sum(s))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_K_Wg7swfFZ","colab_type":"text"},"source":["Given that the output `s`, how do I find out the index for the highest probability? (HINT: use `np.argmax()`)"]},{"cell_type":"code","metadata":{"id":"bkJzMElfwxkk","colab_type":"code","colab":{}},"source":["# Find index within s containing the highest prob\n","?"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sHRz-5FYfVJT","colab_type":"text"},"source":["# TensorFlow\n","\n","## Understanding graphs\n","\n","Now we're set to convert this knowledge into TensorFlow code to train a basic classifier! To start, we need to understand just a little bit about how TensorFlow works. To train network, we first need to define the necessary operations in a structure known as a **graph**. A graph simply represents the series of instructions that dictate what happens to our input data as it *flows* through the graph. Keep in mind that while defining a graph, nothing is actually being calculated yet; we are simply creating a template or blueprint for what *should* happen once data starts to be passed through the graph.\n","\n","## Placeholders\n","\n","To begin, we first define what is known as a **placeholder**. A **placeholder** defines where data will be placed into the graph. To define a **placeholder** we must know:\n","\n","1. The type of data we're using. The two most popular types of data are *floating point* (e.g. decimals) or *integers*. Since neural network weights are finely tuned, they are usually represented by *floating point* decimals, and will be the type of data we use for all our projects.\n","2. The size of data we're using. Sometimes the size along a particular matrix dimension may be unknown, in which case we can use the Python `None` variable.\n","\n","Let's see a few examples:"]},{"cell_type":"code","metadata":{"id":"iJP8runQgyLZ","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","# Integer matrix of size 4 x 2\n","im = tf.placeholder(tf.int32, [4, 2])\n","\n","# Floating point matrices of size 3 x 3\n","im = tf.placeholder(tf.float32, [3, 3])\n","\n","# Floating point matrices of ? x 3 size\n","im = tf.placeholder(tf.float32, [None, 3])\n","\n","# Floating point matrices of size 1 x 10\n","im = tf.placeholder(tf.float32, [1, 10])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x4phbKjiiZXV","colab_type":"text"},"source":["## Variables\n","\n","Now let's define a single variable for weights. This variable is defined using `tf.Variable()` instead of a placeholder because we want TensorFlow to keep track of this variable for us and manipulate it over time (e.g. change the weights values as the network is learning). By contrast, the placeholder is something we need to provide to the model and which stays constant during the learning process. \n","\n","To define a new variable, simply pass the value of whatever out want the new variable to be initialized as:"]},{"cell_type":"code","metadata":{"id":"sEV6XDfUjb4j","colab_type":"code","colab":{}},"source":["# Define weights in Numpy\n","weights = np.random.normal(loc=0, scale=0.0001, size=(10, 5))\n","\n","# Define weights in Tensorflow\n","weights = tf.Variable(weights.astype('float32'))\n","\n","print(weights.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZYmYhQplMW5","colab_type":"text"},"source":["## Matrix multiply\n","\n","The syntax for a matrix multiply is very similar in TensorFlow compared by NumPy:"]},{"cell_type":"code","metadata":{"id":"7AiXZgonldZ9","colab_type":"code","colab":{}},"source":["logits = tf.matmul(im, weights)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fD0zLnOql5j9","colab_type":"text"},"source":["What is size of the resulting logits matrix?"]},{"cell_type":"code","metadata":{"id":"zXPwpYsyl-JS","colab_type":"code","colab":{}},"source":["print(logits.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F3pezI1Ml_6U","colab_type":"text"},"source":["## Running a graph\n","\n","Now that we've defined a simple \"graph\" (composing of a grand total of one operation), let's go ahead and \"run\" this model. To do so, we need to tell TensorFlow to create the graph for us. This is done by creating a `tf.InteractiveSession()` object and initializing all the variables:"]},{"cell_type":"code","metadata":{"id":"uAVPbgakniv0","colab_type":"code","colab":{}},"source":["sess = tf.InteractiveSession()\n","tf.global_variables_initializer().run()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jQvqwqNgnrKH","colab_type":"text"},"source":["Alright, now we have a graph ready for us to interact with. To pass data through the graph (e.g. let our input matrices/tensors *flow* through the graph) we use the `sess.run()` method tells us which output in the graph we would like to extract. This method requires the creation of what is known as a **feed_dict** (a Python dictionary) where each dictionary *key* contains a placeholder variable, and the corresponding dictionary *value* contains the data you would like to place inside that placeholder. In our simple example, the only placeholder we have is the `im` variable which we defined as a *float* matrix of size 1 x 10 (see above). An example **feed_dict** dictionary would thus look like this:"]},{"cell_type":"code","metadata":{"id":"frPN1PcXpcIQ","colab_type":"code","colab":{}},"source":["# Let's feed a random number image\n","feed_dict = {im: np.random.rand(1, 10)}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6MWoI6Pdp0BZ","colab_type":"text"},"source":["To use `sess.run()`, simply include:\n","\n","1. The output(s) in the graph you would like to extract.\n","2. A valid **feed_dict**."]},{"cell_type":"code","metadata":{"id":"S9HnWr8woRcL","colab_type":"code","colab":{}},"source":["# Extract the input image\n","output = sess.run(im, feed_dict)\n","print('This is the input image')\n","print(output)\n","\n","# Extract the weights\n","output = sess.run(weights, feed_dict)\n","print('This is the weights')\n","print(output)\n","\n","# Extract the logits \n","print('This is the logits')\n","output = sess.run(logits, feed_dict)\n","print(output)\n","\n","# Extract all values in one call\n","outputs = sess.run([im, weights, logits], feed_dict)\n","print('This is the input image')\n","print(outputs[0])\n","print('This is the input weights')\n","print(outputs[1])\n","print('This is the logits')\n","print(outputs[2])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYebTRAYrli6","colab_type":"text"},"source":["## Defining Loss\n","\n","The last thing we need to do is to mathematically define how good our model weights are at any given time. We will do this with a formula known as the softmax cross-entropy function, which is really much simpler than it sounds. Recall that softmax probablities from above are simply the conversion of raw logit scores into a valid probability distribution (sum equals 1). For example, a valid softmax probability distribution may be:\n","\n","```\n","softmax = [0.2, 0.3, 0.5]\n","```\n","\n","If out of three possible outcomes the last one is the correct answer, then the *optimal* target distribution is:\n","\n","```\n","target = [0.0, 0.0, 1.0]\n","```\n","\n","To quantify how good these two distributions match, we use `tf.losses.sparse_softmax_cross_entropy(labels, logits)` where labels represent the correct answer (0, 1, 2, ... etc) and logits represent our raw logits scores. The lower this number, the better our prediction is!\n","\n","To see all of this, let's try building another graph:"]},{"cell_type":"code","metadata":{"id":"Wr9ieJD0tn8Z","colab_type":"code","colab":{}},"source":["# Reset our graph to build a new one\n","tf.reset_default_graph()\n","\n","# Define placeholders for both input image and label\n","im = tf.placeholder(tf.float32, [1, 10])    # example 1 x 10 test images\n","labels = tf.placeholder(tf.int64, [1])      # example single value labels\n","\n","# Define our weights\n","weights = np.random.normal(loc=0, scale=1, size=(10, 5))\n","weights = tf.Variable(weights.astype('float32'))\n","\n","# Define our matmul operation\n","logits = tf.matmul(im, weights)\n","\n","# Define our loss\n","loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n","\n","# Initialize our test graph\n","sess = tf.InteractiveSession()\n","tf.global_variables_initializer().run()\n","\n","# Run with 10 random test examples\n","for i in range(10):\n","    \n","    # Generate random input data and label\n","    rand_im = np.random.rand(1, 10)              # generate random 1 x 10 image\n","    rand_labels = np.random.randint(5, size=(1)) # generate random label between 0 and 4\n","    \n","    # Convert to types matching our defined placeholders\n","    rand_im = rand_im.astype('float32')\n","    rand_labels = rand_labels.astype('int64')\n","    \n","    # Prepare feed_dict\n","    feed_dict = {im: rand_im, labels: rand_labels}\n","    \n","    # Calculate logits and loss  \n","    outputs = sess.run([logits, loss], feed_dict)\n","    \n","    # Use argmax to determine highest logit (model guess)\n","    argmax = np.argmax(outputs[0])\n","    \n","    # Print\n","    print('The answer was: %i | The guess was: %i | The loss was: %0.4f' %\n","         (rand_labels, argmax, outputs[1]))\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zbUtG0_r3odF","colab_type":"text"},"source":["As you can see, correct guesses are rewarded with a lower loss value, whereas incorrect guesses have a higher loss value."]},{"cell_type":"markdown","metadata":{"id":"ifWTHkXWrKIY","colab_type":"text"},"source":["# Building a Model\n","\n","Alright, we now have all the tools to build a classifier together! Let's get started.\n","\n","First, let's create our graph:"]},{"cell_type":"code","metadata":{"id":"qQ11TQoz38wV","colab_type":"code","colab":{}},"source":["def create_model():\n","    \n","    # Reset our graph to build a new one\n","    tf.reset_default_graph()\n","\n","    # ------------------------------------------------------------------------\n","    # Define placeholders for our images and labels\n","    # ------------------------------------------------------------------------\n","    #\n","    # 1. Define images\n","    # \n","    #  - type: float32\n","    #  - size: [None, 784] so that we can feed in as many images as we need\n","    # \n","    # 2. Define labels\n","    # \n","    #  - type: int64\n","    #  - size: [None] so that we can feed in as many labels as we need\n","    # \n","    # ------------------------------------------------------------------------\n","\n","    im = tf.placeholder(?)\n","    labels = tf.placeholder(?)\n","\n","    # ------------------------------------------------------------------------\n","    # Define our weights\n","    # ------------------------------------------------------------------------\n","    # \n","    # 1. Initialize from random distribution with mean = 0, SD = 0.001\n","    # 2. Convert to float32\n","    # 3. Move from NumPy to Tensorflow with tf.Variable()\n","    # \n","    # HINT: what size matrix do we need to connect an input image to 10 outputs?\n","    # \n","    # ------------------------------------------------------------------------\n","\n","    weights = np.random.normal(?)\n","    weights = tf.Variable(weights.astype(?))\n","\n","    # ------------------------------------------------------------------------\n","    # Define our matmul operation\n","    # ------------------------------------------------------------------------\n","\n","    logits = tf.matmul(?)\n","\n","    # ------------------------------------------------------------------------\n","    # Define our softmax cross-entropy loss\n","    # ------------------------------------------------------------------------\n","    # \n","    # HINT: use tf.losses.sparse_softmax_cross_entropy() as above\n","    #\n","    # ------------------------------------------------------------------------\n","\n","    loss = tf.losses.sparse_softmax_cross_entropy(?)\n","\n","    # ------------------------------------------------------------------------\n","    # Define our optimizer\n","    # ------------------------------------------------------------------------\n","    # \n","    # An optimizer is a special TensorFlow class that takes your model weights and \n","    # adjusts them ever so slightly so that they will make a better prediction the\n","    # next time around. They are implemented with a technique known as \n","    # backpropogation which we will learn about in further detail during later \n","    # lectures. For now, just know that this is what we are using here.\n","    #\n","    # ------------------------------------------------------------------------\n","\n","    train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n","    \n","    return im, labels, weights, logits, loss, train_op\n","\n","# ------------------------------------------------------------------------\n","# Test our model\n","# ------------------------------------------------------------------------\n","# \n","# If the graph was defined properly, we should be able to check the out\n","# what the model outputs should look like. Can you guess by the shapes\n","# of our logits and losses will be?\n","# \n","# ------------------------------------------------------------------------\n","\n","im, labels, weights, logits, loss, train_op = create_model()\n","print(logits.shape)\n","print(loss.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DN-Fftb972p_","colab_type":"code","colab":{}},"source":["# ------------------------------------------------------------------------\n","# Create our model\n","# ------------------------------------------------------------------------\n","\n","im, labels, weights, logits, loss, train_op = create_model()\n","\n","# ------------------------------------------------------------------------\n","# Initialize our test graph\n","# ------------------------------------------------------------------------\n","# \n","# What two things do we need to initialize our graph?\n","# \n","# ------------------------------------------------------------------------\n","\n","sess = tf.InteractiveSession()\n","tf.global_variables_initializer().run()\n","\n","# ------------------------------------------------------------------------\n","# Train our algorithm \n","# ------------------------------------------------------------------------\n","# \n","# Let's set up a loop to train our algorithm by feeding it data iteratively.\n","# For each iteration, we will feed a batch_size number of images into our \n","# model and let it readjust it's neuronal weights.\n","# \n","# ------------------------------------------------------------------------\n","\n","iterations = 2000 \n","batch_size = 256 \n","accuracies = []\n","losses = []\n","\n","for i in range(iterations):\n","    \n","    # --------------------------------------------------------------------\n","    # Grab a total of batch_size number of random images and labels \n","    # --------------------------------------------------------------------\n","    # \n","    # 1. Pick batch_size number of random indices between 0 and 60,000\n","    # 2. Select those images / labels\n","    #\n","    # --------------------------------------------------------------------\n","    \n","    rand_indices = np.random.randint(?, size=?)\n","    x_batch = x[rand_indices]\n","    y_batch = y[rand_indices]\n","    \n","    # --------------------------------------------------------------------\n","    # Normalize x_batch\n","    # --------------------------------------------------------------------\n","    # \n","    # Currently, values in x range from 0 to 255. If we normalize these values\n","    # to a mean of 0 and SD of 1 we will improve the stability of training\n","    # and furthermore improve interpretation of learned weights. Use the\n","    # following code to normalize your batch:\n","    # \n","    # --------------------------------------------------------------------\n","    \n","    x_batch = (x_batch - np.mean(x_batch)) / np.std(x_batch)\n","    \n","    # Convert to types matching our defined placeholders\n","    # HINT: integer, float... ?\n","    x_batch = x_batch.astype(?)\n","    y_batch = y_batch.astype(?)\n","    \n","    # Prepare feed_dict\n","    feed_dict = {?}\n","     \n","    # --------------------------------------------------------------------\n","    # Run training iteration via sess.run()\n","    # --------------------------------------------------------------------\n","    # \n","    # Here, in addition to whichever ouputs we wish to extract, we need to\n","    # also include the train_op variable. Including train_op will tell \n","    # Tensorflow that in addition to calculating the intermediates of our graph,\n","    # we also need to readjust the variables so that the overall loss goes\n","    # down.\n","    # \n","    # --------------------------------------------------------------------\n","    \n","    outputs = sess.run([logits, loss, train_op], feed_dict)\n","    \n","    # --------------------------------------------------------------------\n","    # Use argmax to determine highest logit (model guess)\n","    # --------------------------------------------------------------------\n","    # \n","    # Keep in mind our logits matrix is (batch_size x 10) in size representing\n","    # a total of batch_size number of predictions. How do we process this matrix\n","    # with the np.argmax() to find the highest logit along each row of the matrix\n","    # (e.g. find the prediction for each of our images)?\n","    # \n","    # HINT: what does the axis parameter in np.argmax(a, axis) specify?\n","    # \n","    # --------------------------------------------------------------------\n","    \n","    predictions = np.argmax(?)\n","    \n","    # --------------------------------------------------------------------\n","    # Calculate accuracy \n","    # --------------------------------------------------------------------\n","    # \n","    # Consider the following:\n","    # \n","    # - predictions = the predicted digits\n","    # - y_batch = the ground-truth digits\n","    # \n","    # How do I calculate an accuracy % with this data?\n","    # \n","    # --------------------------------------------------------------------\n","    \n","    accuracy = ? \n","    \n","    # --------------------------------------------------------------------\n","    # Accumulate and print iteration, loss and accuracy \n","    # --------------------------------------------------------------------\n","    \n","    # This line of code will print progress\n","    print('Iteration %05i | Loss = %07.3f | Accuracy = %0.4f' %\n","        (i + 1, outputs[1], accuracy))\n","    \n","    # Append outputs[1] to losses list\n","    losses.append(outputs[1])\n","    \n","    # Append accuracy to accuracies list\n","    accuracie.append(accuracy)\n","    \n","# --------------------------------------------------------------------\n","# Graph outputs and accuracy\n","# --------------------------------------------------------------------\n","pylab.plot(losses)\n","pylab.title('Model loss over time')\n","pylab.show()\n","\n","pylab.plot(accuracies)\n","pylab.title('Model accuracy over time')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UHHFD9Y-Nqac","colab_type":"text"},"source":["Congratulations! If that went well, you should have trained a network to predict digits. Even though it was extremely simple, it was able to predict with about 90% accuracy. That's really impressive, considering how difficult this would be with hand-crafted rules.\n","\n","## Model weights\n","\n","Recall our hypothesis about the final \"appearance\" of our trained model weights, which are in essence expected to learn what amounts to an *average filter* of the training data. Does this hold true?"]},{"cell_type":"code","metadata":{"id":"AXXQMw_9Nbzg","colab_type":"code","colab":{}},"source":["# --------------------------------------------------------------------\n","# Extract model weights \n","# --------------------------------------------------------------------\n","# \n","# What line of code do we need here to extract the model weights?\n","# \n","# HINT: what do we pass to sess.run()?\n","#\n","# --------------------------------------------------------------------\n","\n","W = sess.run(?)\n","\n","# Use this code to visualize weights\n","fig = pylab.figure()\n","for i in range(10):\n","    fig.add_subplot(2, 5, i + 1)\n","    pylab.axis('off')\n","    pylab.imshow(W[..., i].reshape(28, 28))\n","    \n","pylab.show()"],"execution_count":0,"outputs":[]}]}